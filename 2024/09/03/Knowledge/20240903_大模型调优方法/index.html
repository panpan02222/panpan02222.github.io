<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/ghost-32-32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/ghost-16-16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="大语言模型调优方案，涉及计算效能调优 , 推理效果调优 , 模型结构调优">
<meta property="og:type" content="article">
<meta property="og:title" content="大语言模型调优方案">
<meta property="og:url" content="http://example.com/2024/09/03/Knowledge/20240903_%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98%E6%96%B9%E6%B3%95/index.html">
<meta property="og:site_name" content="潘秉宏的博客">
<meta property="og:description" content="大语言模型调优方案，涉及计算效能调优 , 推理效果调优 , 模型结构调优">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/LLM-tuning-01.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-02.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-03.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-04.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-25.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-24.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-07.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-05.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-06.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-08.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-09.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-10.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-11.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-12.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-13.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-14.png">
<meta property="og:image" content="http://example.com/LLM-tuning-17.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-15.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-16.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-18.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-26.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-19.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-20.png">
<meta property="og:image" content="http://example.com/img/LLM-tuning-21.png">
<meta property="og:image" content="http://example.com/LLM-tuning-22.png">
<meta property="og:image" content="http://example.com/LLM-tuning-23.png">
<meta property="article:published_time" content="2024-09-03T06:10:31.000Z">
<meta property="article:modified_time" content="2024-09-05T06:29:41.463Z">
<meta property="article:author" content="潘秉宏">
<meta property="article:tag" content="llm">
<meta property="article:tag" content="train">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/LLM-tuning-01.png">


<link rel="canonical" href="http://example.com/2024/09/03/Knowledge/20240903_%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98%E6%96%B9%E6%B3%95/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2024/09/03/Knowledge/20240903_%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98%E6%96%B9%E6%B3%95/","path":"2024/09/03/Knowledge/20240903_大模型调优方法/","title":"大语言模型调优方案"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>大语言模型调优方案 | 潘秉宏的博客</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>
<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">潘秉宏的博客</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%95%88%E8%B0%83%E4%BC%98"><span class="nav-number">1.</span> <span class="nav-text">算效调优</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-GPU%E5%8A%A0%E9%80%9F"><span class="nav-number">1.1.</span> <span class="nav-text">1. GPU 加速</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-%E4%B8%BA%E4%BB%80%E4%B9%88GPU%E5%8F%AF%E4%BB%A5%E5%AF%B9%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%89%E5%8A%A0%E9%80%9F%E6%95%88%E6%9E%9C"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1 为什么 GPU 可以对大模型有加速效果？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-GPU%E9%87%8C%E6%9C%89%E4%BB%80%E4%B9%88"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2 GPU 里有什么，？</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#CUDA-Cores"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">CUDA Cores</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Tensor-Cores"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">Tensor Cores</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#CUDA-Cores-Tensor-Cores%E5%8C%BA%E5%88%AB"><span class="nav-number">1.1.2.3.</span> <span class="nav-text">CUDA Cores&amp;Tensor Cores 区别</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-%E4%B8%BB%E6%B5%81GPU%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.3 主流 GPU 性能对比</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-%E8%AE%AD%E7%BB%83-%E6%8E%A8%E7%90%86%E6%9C%80%E4%BD%B3%E9%85%8D%E7%BD%AE"><span class="nav-number">1.1.4.</span> <span class="nav-text">1.4 训练 &#x2F; 推理最佳配置</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-TPU-FPGA"><span class="nav-number">1.2.</span> <span class="nav-text">2. TPU&#x2F;FPGA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#TPU"><span class="nav-number">1.2.1.</span> <span class="nav-text">TPU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#FPGA"><span class="nav-number">1.2.2.</span> <span class="nav-text">FPGA</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%86%85%E5%AD%98%E5%B8%A6%E5%AE%BD"><span class="nav-number">1.3.</span> <span class="nav-text">3. 内存带宽</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-%E5%86%85%E5%AD%98%E5%B8%A6%E5%AE%BD%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="nav-number">1.3.1.</span> <span class="nav-text">3.1 内存带宽的重要性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-%E9%AB%98%E9%80%9F%E5%86%85%E5%AD%98%E6%8A%80%E6%9C%AF"><span class="nav-number">1.3.2.</span> <span class="nav-text">3.2 高速内存技术</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%BE%E7%BD%AE%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5"><span class="nav-number">1.3.3.</span> <span class="nav-text">3.3 分布式设置中的网络连接</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%AD%98%E5%82%A8"><span class="nav-number">1.4.</span> <span class="nav-text">4. 存储</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-%E4%BC%A0%E7%BB%9F%E5%AD%98%E5%82%A8%E6%96%B9%E6%A1%88"><span class="nav-number">1.4.1.</span> <span class="nav-text">4.1 传统存储方案</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#HDD-%E6%9C%BA%E6%A2%B0%E7%A1%AC%E7%9B%98"><span class="nav-number">1.4.1.1.</span> <span class="nav-text">HDD (机械硬盘)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SSD-%E5%9B%BA%E6%80%81%E7%A1%AC%E7%9B%98"><span class="nav-number">1.4.1.2.</span> <span class="nav-text">SSD (固态硬盘)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-%E9%AB%98%E9%80%9F%E5%AD%98%E5%82%A8%E6%96%B9%E6%A1%88"><span class="nav-number">1.4.2.</span> <span class="nav-text">4.2 高速存储方案</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#NVMe-SSD-Non-Volatile-Memory-Express"><span class="nav-number">1.4.2.1.</span> <span class="nav-text">NVMe SSD(Non-Volatile Memory Express)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.4.3.</span> <span class="nav-text">4.3 存储系统与大模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E5%86%B7%E5%8D%B4%E7%B3%BB%E7%BB%9F"><span class="nav-number">1.5.</span> <span class="nav-text">5. 冷却系统</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-%E9%A3%8E%E5%86%B7"><span class="nav-number">1.5.1.</span> <span class="nav-text">5.1 风冷</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-%E6%B6%B2%E5%86%B7"><span class="nav-number">1.5.2.</span> <span class="nav-text">5.2 液冷</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-%E7%83%AD%E9%80%9A%E9%81%93-%E5%86%B7%E9%80%9A%E9%81%93%E5%88%86%E7%A6%BB"><span class="nav-number">1.5.3.</span> <span class="nav-text">5.3  热通道 &#x2F; 冷通道分离</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E5%85%B6%E4%BB%96"><span class="nav-number">1.6.</span> <span class="nav-text">6. 其他</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-%E9%AB%98%E6%95%88%E7%94%B5%E6%BA%90%E4%BE%9B%E5%BA%94%E8%AE%BE%E5%A4%87"><span class="nav-number">1.6.1.</span> <span class="nav-text">6.1 高效电源供应设备</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-%E5%8A%A8%E6%80%81%E5%8A%9F%E8%80%97%E7%AE%A1%E7%90%86%E6%8A%80%E6%9C%AF"><span class="nav-number">1.6.2.</span> <span class="nav-text">6.2 动态功耗管理技术</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-%E4%B8%8D%E9%97%B4%E6%96%AD%E7%94%B5%E6%BA%90-UPS-%E5%92%8C%E5%86%97%E4%BD%99%E8%AE%BE%E8%AE%A1"><span class="nav-number">1.6.3.</span> <span class="nav-text">6.3 不间断电源 (UPS) 和冗余设计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-%E8%83%BD%E6%BA%90%E7%9B%91%E6%B5%8B%E4%B8%8E%E7%AE%A1%E7%90%86"><span class="nav-number">1.6.4.</span> <span class="nav-text">6.4 能源监测与管理</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BE%93%E5%87%BA%E6%95%88%E6%9E%9C%E8%B0%83%E4%BC%98"><span class="nav-number">2.</span> <span class="nav-text">输出效果调优</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E4%BC%98%E5%8C%96%E6%95%B0%E6%8D%AE"><span class="nav-number">2.1.</span> <span class="nav-text">1. 优化数据</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-%E6%95%B0%E6%8D%AE%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="nav-number">2.1.1.</span> <span class="nav-text">1.1 数据的重要性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-%E6%95%B0%E6%8D%AE%E5%9B%9E%E6%B5%81%E6%96%B9%E6%A1%88"><span class="nav-number">2.1.2.</span> <span class="nav-text">1.2 数据回流方案</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E8%B0%83%E6%95%B4%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-number">2.2.</span> <span class="nav-text">2. 调整超参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B-Prompt-Engineering"><span class="nav-number">2.3.</span> <span class="nav-text">3. 提示词工程  (Prompt Engineering)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90-Retrieval-Augmented-Generation-RAG"><span class="nav-number">2.4.</span> <span class="nav-text">4. 检索增强生成 (Retrieval-Augmented Generation, RAG)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.5.</span> <span class="nav-text">5. 微调预训练模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-%E6%9C%89%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83-SFT"><span class="nav-number">2.5.1.</span> <span class="nav-text">5.1 有监督微调 (SFT)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#5-1-1-%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E9%9C%80%E8%A6%81%E7%94%A8%E5%88%B0SFT"><span class="nav-number">2.5.1.1.</span> <span class="nav-text">5.1.1 什么时候需要用到 SFT ?</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-1-2-SFT%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F"><span class="nav-number">2.5.1.2.</span> <span class="nav-text">5.1.2 SFT 数据格式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-1-3-SFT%E5%BE%AE%E8%B0%83%E6%89%80%E9%9C%80%E6%95%B0%E6%8D%AE%E9%87%8F%E7%BA%A7"><span class="nav-number">2.5.1.3.</span> <span class="nav-text">5.1.3 SFT 微调所需数据量级</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-LoRA%E5%BE%AE%E8%B0%83"><span class="nav-number">2.5.2.</span> <span class="nav-text">5.2 LoRA 微调</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#5-2-1-%E4%BB%80%E4%B9%88%E6%98%AFLoRA"><span class="nav-number">2.5.2.1.</span> <span class="nav-text">5.2.1 什么是 LoRA ?</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-2-2-LoRA%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="nav-number">2.5.2.2.</span> <span class="nav-text">5.2.2 LoRA 微调方法的基本原理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-2-3-LoRA%E5%BE%AE%E8%B0%83%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="nav-number">2.5.2.3.</span> <span class="nav-text">5.2.3 LoRA 微调的优势</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-2-4-QLoRA%E5%BE%AE%E8%B0%83"><span class="nav-number">2.5.2.4.</span> <span class="nav-text">5.2.4 QLoRA 微调</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-Freeze%E5%BE%AE%E8%B0%83"><span class="nav-number">2.5.3.</span> <span class="nav-text">5.3 Freeze 微调</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-4-GaLore%E5%BE%AE%E8%B0%83"><span class="nav-number">2.5.4.</span> <span class="nav-text">5.4 GaLore 微调</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E4%BB%A3%E7%90%86-Agent"><span class="nav-number">2.6.</span> <span class="nav-text">6. 代理 (Agent)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">3.</span> <span class="nav-text">模型结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%A8%A1%E5%9E%8B%E6%B7%B1%E5%BA%A6%E4%B8%8E%E5%AE%BD%E5%BA%A6%E7%9A%84%E5%B9%B3%E8%A1%A1"><span class="nav-number">3.1.</span> <span class="nav-text">1. 模型深度与宽度的平衡</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5"><span class="nav-number">3.2.</span> <span class="nav-text">2. 残差连接</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">3.3.</span> <span class="nav-text">3. 注意力机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-number">3.4.</span> <span class="nav-text">4. 多头注意力</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Transformer%E6%9E%B6%E6%9E%84"><span class="nav-number">3.5.</span> <span class="nav-text">5. Transformer 架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="nav-number">3.6.</span> <span class="nav-text">6. 正则化技术</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6-Mixture-of-Experts-MoE"><span class="nav-number">3.7.</span> <span class="nav-text">7 混合专家 (Mixture of Experts, MoE)</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="潘秉宏"
      src="/images/avatar.gif">
  <!--
  <p class="site-author-name" itemprop="name">潘秉宏</p>
  -->
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">39</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/panpan02222" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;panpan02222" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://19909442097@163.com/" title="E-Mail → https:&#x2F;&#x2F;19909442097@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/09/03/Knowledge/20240903_%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98%E6%96%B9%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="潘秉宏">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘秉宏的博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="大语言模型调优方案 | 潘秉宏的博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          大语言模型调优方案
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-09-03 14:10:31" itemprop="dateCreated datePublished" datetime="2024-09-03T14:10:31+08:00">2024-09-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-09-05 14:29:41" itemprop="dateModified" datetime="2024-09-05T14:29:41+08:00">2024-09-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Knowledge/" itemprop="url" rel="index"><span itemprop="name">Knowledge</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>29 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>大语言模型调优方案，涉及<strong>计算效能调优</strong> , <strong>推理效果调优</strong> , <strong>模型结构调优</strong></p>
<span id="more"></span>

<h2 id="算效调优"><a href="#算效调优" class="headerlink" title="算效调优"></a>算效调优</h2><p>硬件层面各个部件对大模型的优化策略</p>
<h3 id="1-GPU加速"><a href="#1-GPU加速" class="headerlink" title="1. GPU加速"></a>1. GPU 加速</h3><h4 id="1-1-为什么GPU可以对大模型有加速效果"><a href="#1-1-为什么GPU可以对大模型有加速效果" class="headerlink" title="1.1 为什么GPU可以对大模型有加速效果?"></a>1.1 为什么 GPU 可以对大模型有加速效果？</h4><p>GPU 的核心优势在于其并行处理能力，可以同时执行成千上万的计算任务。对于深度学习模型而言，这意味着可以并行处理大量的矩阵乘法和向量运算，这些是模型训练的核心。GPU 的计算能力通常以 TFLOPS（每秒万亿次浮点运算）来衡量。高 TFLOPS 值意味着 GPU 能够在较短的时间内完成更多的计算任务，从而加快模型的训练速度。</p>
<p style="text-align: center;">Tesla V100 Accelerator (Front)</p>

<p><img src="/../img/LLM-tuning-01.png" alt="image-20240905084258878"></p>
<p>选择 GPU 而非 CPU 进行大模型训练的主要原因是因为 GPU 在并行处理能力、高吞吐量和针对机器学习任务的优化方面的优势。这使得 GPU 成为训练复杂和大规模机器学习模型的首选。</p>
<ul>
<li><p><strong>并行处理能力：</strong></p>
<p>GPU 拥有成千上万个较小、更专用的核心，这使得它们能够同时处理多个任务。这种并行处理能力使 GPU 非常适合执行机器学习和深度学习算法中的大量矩阵和向量运算。相比之下，CPU（中央处理单元）核心数量较少，但每个核心的通用计算能力更强，适用于需要大量逻辑和顺序处理的任务。</p>
</li>
<li><p><strong>高吞吐量：</strong></p>
<p>GPU 能够提供更高的吞吐量，这意味着它们可以在较短的时间内处理更多的数据。这对于训练大型模型尤其重要，因为这些模型通常需要处理巨大的数据集，并执行数以亿计的运算。</p>
</li>
<li><p><strong>大规模计算：</strong></p>
<p>GPU 最初是为了处理复杂的图形和<a target="_blank" rel="noopener" href="https://cloud.tencent.com/product/tiia?from_column=20065&amp;from=20065">图像处理</a>任务而设计的，这些任务需要大量的计算和数据处理。这些设计特性也让 GPU 非常适合于训练大型机器学习模型，因为这些模型需要进行大量的数学运算，特别是在训练神经网络时。</p>
</li>
<li><p><strong>优化的库和框架：</strong></p>
<p>许多深度学习框架和库，如 TensorFlow、PyTorch 等，都针对 GPU 进行了优化，以充分利用其并行处理能力。这些优化包括专门的算法和硬件加速技术，可以显著加快模型训练过程。</p>
</li>
<li><p><strong>成本：</strong></p>
<p>虽然高端 GPU 的初始投资可能比 CPU 高，但在处理大规模机器学习任务时，GPU 因其较高的效率和速度，可以提供更好的成本效益。尤其是在云计算环境中，用户可以根据需要临时租用 GPU 资源，进一步提高成本效益。</p>
</li>
</ul>
<h4 id="1-2-GPU里有什么"><a href="#1-2-GPU里有什么" class="headerlink" title="1.2 GPU里有什么,?"></a>1.2 GPU 里有什么，？</h4><p>Tensor Cores 和 CUDA Cores 都是 NVIDIA GPU 架构中的关键组成部分，但它们的设计目标和服务的对象有所不同。下面详细介绍这两种核心的区别：</p>
<h5 id="CUDA-Cores"><a href="#CUDA-Cores" class="headerlink" title="CUDA Cores"></a>CUDA Cores</h5><p>CUDA Cores 是 NVIDIA GPU 中的基础计算单元，类似于 CPU 中的核心，但专门为并行计算而优化。CUDA Cores 能够执行各种类型的数学运算，包括整数运算、单精度浮点运算以及双精度浮点运算。CUDA Cores 的数量决定了 GPU 的并行计算能力，更多的 CUDA Cores 意味着更强的并行处理能力。</p>
<p>CUDA Cores 被设计为一种通用的计算资源，可以用于执行广泛的任务，从简单的图形渲染到复杂的科学计算，甚至是深度学习模型的训练。CUDA Cores 支持通过 CUDA 编程接口直接访问，使得开发人员能够编写高效的并行计算代码。</p>
<h5 id="Tensor-Cores"><a href="#Tensor-Cores" class="headerlink" title="Tensor Cores"></a>Tensor Cores</h5><p>Tensor Cores 是 NVIDIA 为加速深度学习任务而专门设计的一种新型计算单元。它们最早出现在 2017 年的 Volta 架构中，并随后在 Turing、Ampere 等架构中得到了发展和完善。Tensor Cores 的主要特点是它们特别适合执行深度学习所需的矩阵运算，如矩阵乘法和累积运算。</p>
<p>Tensor Cores 的一个重要特性是它们支持混合精度计算，即能够在 FP16（半精度浮点数）和 TF32（Tensor Float-32）之间进行切换，从而提供更高的计算效率和能效比。此外，Tensor Cores 还能在每个时钟周期内执行多项操作，相比之下，传统的 CUDA Cores 在每个时钟周期只能执行单一操作。</p>
<h5 id="CUDA-Cores-Tensor-Cores区别"><a href="#CUDA-Cores-Tensor-Cores区别" class="headerlink" title="CUDA Cores&amp;Tensor Cores区别"></a>CUDA Cores&amp;Tensor Cores 区别</h5><ol>
<li><strong>应用场景</strong>：CUDA Cores 是通用的并行计算单元，可以处理各种计算任务；而 Tensor Cores 则专门针对深度学习中的矩阵运算进行了优化。</li>
<li><strong>计算精度</strong>：CUDA Cores 支持更广泛的精度计算，包括 FP64、FP32 和 INT32 等；Tensor Cores 则专注于半精度浮点数（FP16）和混合精度计算（如 TF32）。</li>
<li><strong>性能</strong>：在处理深度学习相关的矩阵运算时，Tensor Cores 相比 CUDA Cores 能够提供更高的性能和能效比。</li>
</ol>
<p>CUDA Cores 提供了广泛的计算灵活性，而 Tensor Cores 则是在特定任务上（如深度学习）实现了性能的飞跃。这两种核心的组合使得现代 GPU 既能满足传统计算需求，也能适应日益增长的人工智能计算需求。</p>
<h4 id="1-3-主流GPU性能对比"><a href="#1-3-主流GPU性能对比" class="headerlink" title="1.3 主流GPU性能对比"></a>1.3 主流 GPU 性能对比</h4><p><img src="/../img/LLM-tuning-02.png"></p>
<h4 id="1-4-训练-推理最佳配置"><a href="#1-4-训练-推理最佳配置" class="headerlink" title="1.4 训练/推理最佳配置"></a>1.4 训练 / 推理最佳配置</h4><p style="text-align: center;">训练最优配置</p>

<p><img src="/../img/LLM-tuning-03.png" alt="image-20240905090148556"></p>
<p style="text-align: center;">推理最优配置</p>

<p><img src="/../img/LLM-tuning-04.png" alt="image-20240905090325642"></p>
<h3 id="2-TPU-FPGA"><a href="#2-TPU-FPGA" class="headerlink" title="2. TPU/FPGA"></a>2. TPU/FPGA</h3><p>TPU（Tensor Processing Unit）和 FPGA（Field-Programmable Gate Array）都是为加速机器学习任务而设计的专用硬件。</p>
<h4 id="TPU"><a href="#TPU" class="headerlink" title="TPU"></a>TPU</h4><p><img src="/../img/LLM-tuning-25.png" alt="tpu-768x517"></p>
<p>TPU 是由 Google 开发的一种 ASIC（Application-Specific Integrated Circuit），专门针对 TensorFlow 框架进行了优化。TPU 的设计目标是在处理矩阵运算时提供更高的效率，这对于深度学习模型来说是非常关键的，因为它们通常包含大量的矩阵乘法操作。利用 TPU 可以实现以下优化：</p>
<ul>
<li><strong>高效计算</strong>：TPU 能够提供比传统 CPU 或 GPU 更高的浮点运算性能，在处理大规模神经网络时尤其有效。</li>
<li><strong>低精度支持</strong>：TPU 支持 8 位整数运算，这减少了数据传输量，加快了计算速度，同时降低了功耗。</li>
<li><strong>分布式训练</strong>：通过构建 TPU Pod，可以实现多个 TPU 之间的高效通信，从而支持更大规模的数据并行训练。</li>
<li><strong>自动优化</strong>：TPU 编译器可以自动将 TensorFlow 图转化为高效的 TPU 指令序列，减少了手动优化的需要。</li>
</ul>
<h4 id="FPGA"><a href="#FPGA" class="headerlink" title="FPGA"></a>FPGA</h4><p><img src="/../img/LLM-tuning-24.png" alt="fpga-beginners-guide"></p>
<p>FPGA 是一种可编程逻辑器件，可以在硬件级别上根据特定的任务重新配置。这种灵活性使得 FPGA 非常适合于那些需要定制化处理流程的任务。对于大模型的优化，FPGA 提供了如下优势：</p>
<ul>
<li><strong>定制化</strong>：FPGA 可以根据特定算法的需求进行编程，这意味着它可以针对特定的模型架构进行优化，达到最佳的性能。</li>
<li><strong>低延迟</strong>：FPGA 可以实现低延迟的数据处理，这对于实时应用非常重要。</li>
<li><strong>能效比</strong>：相比于 GPU，FPGA 在某些任务上可以提供更好的能效比，尤其是在需要高并发且低功耗的场景下。</li>
<li><strong>灵活性</strong>：FPGA 可以在部署后进行重新配置，以适应新的算法或模型变化。</li>
</ul>
<h3 id="3-内存带宽"><a href="#3-内存带宽" class="headerlink" title="3. 内存带宽"></a>3. 内存带宽</h3><p>提高内存带宽对于优化大模型的性能至关重要。大模型，特别是深度学习模型，通常包含数百万甚至数十亿的参数，这些参数在训练过程中需要频繁地被访问和更新。随着模型规模的增长，内存带宽成为了限制性能的一个重要因素，因为它直接影响到数据的加载和传输速度。</p>
<h4 id="3-1-内存带宽的重要性"><a href="#3-1-内存带宽的重要性" class="headerlink" title="3.1 内存带宽的重要性"></a>3.1 内存带宽的重要性</h4><p>内存带宽是指单位时间内可以从内存读取或写入的最大数据量。对于大模型而言，更高的内存带宽意味着模型可以更快地加载数据进行处理，进而提高计算效率。在大模型的训练过程中，内存带宽的不足会导致数据传输成为瓶颈，从而减慢训练速度。例如，在模型推理过程中，较大的内存缓存可以提高推理速度，但同时也减少了可用的上下文长度，需要在速度和上下文长度之间进行权衡。</p>
<h4 id="3-2-高速内存技术"><a href="#3-2-高速内存技术" class="headerlink" title="3.2 高速内存技术"></a>3.2 高速内存技术</h4><p style="text-align: center">AMD Fiji 首款使用高带宽内存技术的图形处理器</p>

<p><img src="/../img/LLM-tuning-07.png" alt="image-20240905091319693"></p>
<p>高速内存技术，如 HBM（High Bandwidth Memory）和 GDDR6。HBM 通过垂直堆叠 DRAM 芯片来提供更高的带宽，而 GDDR6 则专为高性能图形处理单元（GPU）设计，能够提供比传统 DDR 内存更高的带宽。这些技术特别适用于处理大型数据集和复杂的神经网络模型，因为它们能更有效地支持大量数据的快速读取和写入。</p>
<p><img src="/../img/LLM-tuning-05.png" alt="image-20240905091113941"></p>
<p><img src="/../img/LLM-tuning-06.png" alt="image-20240905091219597"></p>
<h4 id="3-3-分布式设置中的网络连接"><a href="#3-3-分布式设置中的网络连接" class="headerlink" title="3.3 分布式设置中的网络连接"></a>3.3 分布式设置中的网络连接</h4><p>在分布式训练环境中，除了本地内存带宽之外，网络连接的质量也至关重要。快速的网络连接（如 InfiniBand 或 RDMA over Converged Ethernet, RoCE）能够确保数据在多个计算节点间快速、高效地流动。例如，InfiniBand 网络提供的带宽相较于其他网络技术有明显的优势，为解决 AI 大模型对服务器集群中每个 GPU 之间的高速、无缝通信的需求提供了支持。这对于跨多个节点同步模型参数和梯度更新尤为重要。</p>
<p><img src="/../img/LLM-tuning-08.png" alt="image-20240905092221631"></p>
<p>提高内存带宽对于优化大模型的性能有着不可忽视的作用。通过采用高速内存技术和高效的网络连接方案，并结合合理的硬件选择和软件优化策略，可以有效克服内存带宽带来的瓶颈，从而加速大模型的训练和推理过程。</p>
<h3 id="4-存储"><a href="#4-存储" class="headerlink" title="4. 存储"></a>4. 存储</h3><h4 id="4-1-传统存储方案"><a href="#4-1-传统存储方案" class="headerlink" title="4.1 传统存储方案"></a>4.1 传统存储方案</h4><ul>
<li><h5 id="HDD-机械硬盘"><a href="#HDD-机械硬盘" class="headerlink" title="HDD(机械硬盘)"></a>HDD (机械硬盘)</h5><p>HDD 使用旋转磁盘和磁头来存储和读取数据，数据存储在旋转的盘片上，磁头通过移动到正确的轨道上来读取或写入数据。HDD 的优点在于成本较低，能够提供较大的存储空间，适用于存储大量数据且对访问速度要求不高的场景。然而，由于存在机械运动，HDD 的读写速度受限于寻道时间和旋转延迟，导致访问速度较慢，并且在面对大量随机读写请求时性能会下降。此外，机械部件容易因震动或其他物理因素损坏，可靠性较低。</p>
</li>
<li><h5 id="SSD-固态硬盘"><a href="#SSD-固态硬盘" class="headerlink" title="SSD(固态硬盘)"></a>SSD (固态硬盘)</h5><p>  SSD 使用非易失性存储器（通常是 NAND 型闪存）来存储数据，没有活动部件。这使得 SSD 具有快速访问的优势，没有机械延迟，读写速度远高于 HDD，并且抗震动能力强，更加耐用。此外，SSD 的低功耗特点使其在移动设备和数据中心中广泛应用。然而，SSD 的成本通常高于同等容量的 HDD，并且写入寿命有限，尽管近年来有所改进但仍需考虑长期使用中的耐久性问题。</p>
<p>  <img src="/../img/LLM-tuning-09.png"></p>
</li>
</ul>
<h4 id="4-2-高速存储方案"><a href="#4-2-高速存储方案" class="headerlink" title="4.2 高速存储方案"></a>4.2 高速存储方案</h4><ul>
<li><h5 id="NVMe-SSD-Non-Volatile-Memory-Express"><a href="#NVMe-SSD-Non-Volatile-Memory-Express" class="headerlink" title="NVMe SSD(Non-Volatile Memory Express)"></a>NVMe SSD(Non-Volatile Memory Express)</h5>NVMe SSD 通过 PCIe 总线直接与 CPU 通信，利用闪存技术存储数据。NVMe 协议旨在充分利用高速存储介质的特点，如低延迟和高 I/O 并行性。NVMe SSD 提供了极高的带宽，因为 PCIe 接口比 SATA 或 SAS 接口提供了更高的带宽，使得数据传输速度更快。同时，由于减少了中间环节，NVMe SSD 实现了更低的访问延迟。此外，NVMe 支持多队列和多线程，允许多个读写操作同时进行，极大地提高了 I/O 性能。然而，NVMe SSD 的成本较高，尤其是在高端产品中，并且需要特定的硬件支持，可能存在兼容性问题。</li>
</ul>
<p><img src="/../img/LLM-tuning-10.png" alt="image-20240905094222746"></p>
<h4 id="4-3-存储系统与大模型"><a href="#4-3-存储系统与大模型" class="headerlink" title="4.3 存储系统与大模型"></a>4.3 存储系统与大模型</h4><p>对于大模型的训练和推理来说，存储系统的性能至关重要。HDD 虽然在容量和成本上有优势，但由于其机械结构导致的访问速度慢和可靠性问题，已逐渐被 SSD 所取代。SSD 提供了更快的读写速度和更高的可靠性，但在面对更高要求的应用场景时，如深度学习训练，NVMe SSD 凭借其极高的带宽和低延迟特性成为了优选方案。尽管 NVMe SSD 的成本较高，但对于追求极致性能的应用来说，它是不可或缺的选择。在选择存储方案时，应根据实际需求权衡成本与性能之间的关系，以达到最优的效果。</p>
<h3 id="5-冷却系统"><a href="#5-冷却系统" class="headerlink" title="5. 冷却系统"></a>5. 冷却系统</h3><h4 id="5-1-风冷"><a href="#5-1-风冷" class="headerlink" title="5.1 风冷"></a>5.1 风冷</h4><p>风冷是最常见的数据中心冷却方法之一，它通过强制空气流动来带走设备产生的热量。这种方法相对简单且成本较低，适用于大多数常规服务器和硬件配置。风冷系统的核心组成部分包括风扇、过滤器和合理的气流管理。对于大模型的优化而言，风冷系统在训练初期阶段能够提供足够的冷却能力；然而，在处理更大、更复杂的模型时，风冷系统的局限性开始显现，尤其是在高密度部署的环境下，可能无法有效应对局部热点问题，导致硬件过热，影响计算效率和稳定性。</p>
<h4 id="5-2-液冷"><a href="#5-2-液冷" class="headerlink" title="5.2 液冷"></a>5.2 液冷</h4><p>液冷技术因其高效散热能力和更低的噪音水平而变得越来越受欢迎，主要分为直接接触液冷和间接液冷两种形式。直接接触液冷特别适用于高性能计算和深度学习训练，能够提供更高密度的冷却效果，帮助维持 GPU 和 CPU 等关键部件在一个稳定的温度范围内工作，从而确保训练过程的连续性和计算资源的有效利用。液冷还能减少因过热导致的硬件损坏风险，延长硬件寿命，对大模型的优化具有显著优势。</p>
<h4 id="5-3-热通道-冷通道分离"><a href="#5-3-热通道-冷通道分离" class="headerlink" title="5.3  热通道/冷通道分离"></a>5.3  热通道 / 冷通道分离</h4><p>热通道 / 冷通道分离通过物理手段将机房内的气流分为冷通道和热通道两部分，前者专门为服务器提供冷空气，后者则收集热空气并将其引导至冷却设备或空调系统进行处理。这种设计可以显著减少混合冷热空气的情况，提高冷却系统的效能，减少能源消耗，并延长硬件使用寿命。对于大模型而言，热通道 / 冷通道分离有助于保持计算节点的温度稳定，确保训练过程中不会因为过热而导致性能下降或硬件故障，特别适合于分布式训练环境。</p>
<h3 id="6-其他"><a href="#6-其他" class="headerlink" title="6. 其他"></a>6. 其他</h3><h4 id="6-1-高效电源供应设备"><a href="#6-1-高效电源供应设备" class="headerlink" title="6.1 高效电源供应设备"></a>6.1 高效电源供应设备</h4><p>高效的电源供应设备对于优化大模型训练至关重要。80 Plus 白金或钛金认证的电源供应器能够在不同负载条件下提供至少 80% 以上的效率，最高可达 94%，显著减少能量转换过程中的损耗。数字电源管理技术通过实时调整电源输出，确保在各种负载条件下保持高效运作，从而实现更精细的能量管理，减少能源浪费。</p>
<h4 id="6-2-动态功耗管理技术"><a href="#6-2-动态功耗管理技术" class="headerlink" title="6.2 动态功耗管理技术"></a>6.2 动态功耗管理技术</h4><p>动态功耗管理技术如动态电压和频率调整（DVFS）可以根据实际负载动态调整 CPU 和 GPU 的电压和频率。在低负载时降低电压和频率可以大幅减少功耗，而在高负载时则可以迅速恢复性能。此外，智能休眠模式可以在无活动或低负载时将部分硬件单元暂时置于休眠状态，进一步节省能源，确保大模型训练过程中能源使用的高效性。</p>
<h4 id="6-3-不间断电源-UPS-和冗余设计"><a href="#6-3-不间断电源-UPS-和冗余设计" class="headerlink" title="6.3 不间断电源 (UPS) 和冗余设计"></a>6.3 不间断电源 (UPS) 和冗余设计</h4><p>不间断电源（UPS）系统可以在电网供电中断时立即提供备用电源，确保系统不会因突然断电而停止运行，这对于长时间运行的大模型训练任务至关重要。冗余电源设计通过在关键硬件中部署冗余电源，确保即使其中一个电源故障，系统仍能继续运行，提高系统的可靠性和可用性。</p>
<h4 id="6-4-能源监测与管理"><a href="#6-4-能源监测与管理" class="headerlink" title="6.4 能源监测与管理"></a>6.4 能源监测与管理</h4><p>智能监控系统可以实时监测电源的使用情况，识别异常情况，并及时采取措施，例如调整负载分配以优化能源使用。自动化管理系统可以根据实际需求动态调整电源配置，在非高峰时段减少某些硬件的供电，在高峰期增加供电，确保能源使用的最大化效率，从而提高数据中心的整体能效比。</p>
<hr>
<h2 id="输出效果调优"><a href="#输出效果调优" class="headerlink" title="输出效果调优"></a>输出效果调优</h2><h3 id="1-优化数据"><a href="#1-优化数据" class="headerlink" title="1. 优化数据"></a>1. 优化数据</h3><h4 id="1-1-数据的重要性"><a href="#1-1-数据的重要性" class="headerlink" title="1.1 数据的重要性"></a>1.1 数据的重要性</h4><p>大规模语言模型的开发依赖于广泛而多元的数据资源。研究文献详细阐述了人类在训练 GPT-3 模型时主要利用的数据源，这包括经筛选的 CommonCrawl 数据集、WebText2、Books1、Books2 以及英文版 Wikipedia 等。例如，CommonCrawl 的初始数据量高达 45TB，筛选后仅剩 570GB。通过分词技术处理上述资料，大约产生了 5000 亿个词元。为了确保模型能够利用高品质数据进行学习，GPT-3 的训练过程中根据数据来源的差异调整了采样权重。在完成 3000 亿词元的训练量时，英文版 Wikipedia 的数据平均被循环利用了 3.4 次，而 CommonCrawl 和 Books2 的数据循环使用率分别仅为 0.44 次和 0.43 次。鉴于 CommonCrawl 数据集的筛选工作极为复杂，Meta 公司的研究团队在训练 OP 模型时采纳了结合 RoBERTa、Pile [68] 以及 PushShift.io Reddit 数据的策略。考虑到这些数据集主要以英文为主，OPT 模型也从 CommonCrawl 中提取了一部分非英文数据以丰富训练语料。大型语言模型所需的数据资源大致可分为通用数据和专业数据两类。通用数据涵盖了网页内容、图书、新闻报道、对话文本等，以其庞大的规模、多样性和易于获取的特点，为大型语言模型提供了基础的语言建模和泛化能力。而专业数据则包括多语言资料、科学文献、编程代码以及特定领域的专有信息等，这些在预训练阶段的引入，能够显著增强大型语言模型解决特定任务的能力。</p>
<p style="text-align: center">典型大语言模型所使用数量类型的分布</p>

<p><img src="/../img/LLM-tuning-11.png" alt="image-20240905101311693"></p>
<h4 id="1-2-数据回流方案"><a href="#1-2-数据回流方案" class="headerlink" title="1.2 数据回流方案"></a>1.2 数据回流方案</h4><p>数据回流是指将模型生成的数据或预测结果重新引入到训练流程中，以此来更新模型。这种方法可以用于增强模型的学习能力，尤其是在面对那些随着时间变化而变化的数据集时。通过持续地将新产生的数据反馈给模型，可以让模型适应新的模式或纠正之前的偏差，从而提高其泛化能力和预测准确性。</p>
<h3 id="2-调整超参数"><a href="#2-调整超参数" class="headerlink" title="2. 调整超参数"></a>2. 调整超参数</h3><p>调整超参数是指在训练模型之前选择一组最佳的参数值的过程，这些参数不是直接通过学习过程获得的。超参数包括学习率、批次大小、正则化系数等。合理的超参数设置对于模型的表现至关重要。通常，人们会采用网格搜索、随机搜索或贝叶斯优化等方法来进行超参数的优化，以找到能够最大化模型性能的一组超参数组合。</p>
<p><img src="/../img/LLM-tuning-12.png" alt="image-20240905101432943"></p>
<h3 id="3-提示词工程-Prompt-Engineering"><a href="#3-提示词工程-Prompt-Engineering" class="headerlink" title="3. 提示词工程  (Prompt Engineering)"></a>3. 提示词工程  (Prompt Engineering)</h3><p>提示词工程是指精心设计输入给模型的提示词，以便引导模型产生更符合期望的输出。这种技术特别适用于那些基于自然语言处理的大规模预训练模型。通过调整提示词的内容、结构以及语气，可以显著影响模型生成的结果。例如，在某些场景下，使用更正式的语言或提供更多的上下文信息可以促使模型生成更为准确和连贯的回答。提示词工程还可以用来控制生成内容的风格、语气甚至创造力水平。</p>
<p><img src="/../img/LLM-tuning-13.png" alt="image-20240905101633553"></p>
<h3 id="4-检索增强生成-Retrieval-Augmented-Generation-RAG"><a href="#4-检索增强生成-Retrieval-Augmented-Generation-RAG" class="headerlink" title="4. 检索增强生成(Retrieval-Augmented Generation, RAG)"></a>4. 检索增强生成 (Retrieval-Augmented Generation, RAG)</h3><p>检索增强生成是一种结合了检索技术和生成模型的方法，它允许模型在生成答案或内容时参考一个外部的知识库。这样做的好处是可以利用大量静态或动态的信息来增强生成的质量，特别是在处理那些需要精确信息或最新数据的任务时。RAG 可以提升生成内容的相关性和准确性，尤其是在对话系统、问答系统或文本摘要等领域。</p>
<p><img src="/../img/LLM-tuning-14.png" alt="image-20240905101730828"></p>
<h3 id="5-微调预训练模型"><a href="#5-微调预训练模型" class="headerlink" title="5. 微调预训练模型"></a>5. 微调预训练模型</h3><p>微调预训练模型是指在一个已经预先训练好的模型基础上，针对特定任务进一步训练模型的过程。预训练模型通常是在大规模数据上训练得到的，拥有良好的通用特征提取能力。通过在特定任务的小数据集上继续训练，可以使得模型更加专注于解决该任务，从而提升模型在特定领域或任务上的表现。</p>
<p style="text-align:center">微调各种尺寸大模型的硬件要求</p>

<p><img src="/LLM-tuning-17.png" alt="PixPin_2024-09-05_10-46-18"></p>
<h4 id="5-1-有监督微调-SFT"><a href="#5-1-有监督微调-SFT" class="headerlink" title="5.1 有监督微调(SFT)"></a>5.1 有监督微调 (SFT)</h4><p>在自然语言处理（NLP）领域，Supervised Finetuning（SFT）是一种至关重要的技术手段，用来提升大模型在某一特定领域的表现。通过精细的策划和实施，SFT 能够指导模型的学习过程，确保其学习成果与既定目标高度吻合。</p>
<p>SFT 指的是，用户提供一份标注好的数据集，即，包含输入的 prompt 和预期输出的 response。然后，在已有的某个基座模型上继续调整参数，来达到和下游任务对齐的目的。</p>
<h5 id="5-1-1-什么时候需要用到SFT"><a href="#5-1-1-什么时候需要用到SFT" class="headerlink" title="5.1.1 什么时候需要用到SFT ?"></a>5.1.1 什么时候需要用到 SFT ?</h5><ol>
<li>通过提示词工程无法解决或提示词中描述过于复杂时。</li>
<li>对大模型输出内容有格式要求时，而模型仍有部分条件不符合要求。</li>
<li>期望通过 SFT 来减少 prompt 中的内容，加速线上推理的耗时。</li>
</ol>
<h5 id="5-1-2-SFT数据格式"><a href="#5-1-2-SFT数据格式" class="headerlink" title="5.1.2 SFT数据格式"></a>5.1.2 SFT 数据格式</h5><p><img src="/../img/LLM-tuning-15.png" alt="image-20240905102047209"></p>
<p>每行一条 JSON 格式的数据：</p>
<ul>
<li><p>messages (list, required): 描述一个对话列表。</p>
</li>
<li><p>role (str, required): 角色，system、user、assistant 中的一个。</p>
</li>
<li><p>content (str, required): 对话内容文本。</p>
</li>
<li><p>loss_weight (float, optional): 对于内容的 loss 训练权重。当 role=system/user，loss_weight 默认值为 0.0 且不可修改；当 role=assistant，loss_weight 默认值为 1.0。通过 loss_weight 字段，可以在训练数据中修改默认值。</p>
</li>
</ul>
<h5 id="5-1-3-SFT微调所需数据量级"><a href="#5-1-3-SFT微调所需数据量级" class="headerlink" title="5.1.3 SFT微调所需数据量级"></a>5.1.3 SFT 微调所需数据量级</h5><p>模型中 SFT 的过程中，会学习 prompt 到 response 到映射关系，如果我们 SFT 的数据存在噪声（如错别字、错误格式、不符合预期输出的样本等），那么会对模型的训练过程造成比较严重的影响。因此，不可以一味去堆叠 SFT 的样本数量，样本的质量比数量更重要。</p>
<p>针对不同场景下，数据量级的建议：</p>
<ul>
<li><p>文案生成，剧本创作，小说续写等生成类任务：2～3k。</p>
</li>
<li><p>参考问答：2k ~ 1w。</p>
</li>
<li><p>文本分类：1～3k，和类别数量以及任务难易度强相关，类别较多 / 任务较难的场景可能需要 1w 条以上。</p>
</li>
</ul>
<h4 id="5-2-LoRA微调"><a href="#5-2-LoRA微调" class="headerlink" title="5.2 LoRA微调"></a>5.2 LoRA 微调</h4><h5 id="5-2-1-什么是LoRA"><a href="#5-2-1-什么是LoRA" class="headerlink" title="5.2.1 什么是LoRA ?"></a>5.2.1 什么是 LoRA ?</h5><p>LoRA（Low-Rank Adaptation of Large Language Models），直译为大语言模型的低阶自适应。LoRA 的基本原理是冻结预训练好的模型权重参数，在冻结原模型参数的情况下，通过往模型中加入额外的网络层，并只训练这些新增的网络层参数。由于这些新增参数数量较少，这样不仅 finetune 的成本显著下降，还能获得和全模型参数参与微调类似的效果。</p>
<p>随着大语言模型的发展，模型的参数量越来越大，比如 GPT-3 参数量已经高达 1750 亿，因此，微调所有模型参数变得不可行。LoRA 微调方法由微软提出，通过只微调新增参数的方式，大大减少了下游任务的可训练参数数量。</p>
<h5 id="5-2-2-LoRA微调方法的基本原理"><a href="#5-2-2-LoRA微调方法的基本原理" class="headerlink" title="5.2.2 LoRA微调方法的基本原理"></a>5.2.2 LoRA 微调方法的基本原理</h5><p>神经网络的每一层都包含矩阵的乘法。这些层中的权重矩阵通常具有满秩。当适应特定任务时，预训练语言模型具有低的 “内在维度”，将它们随机投影到更小的子空间时，它们仍然可以有效地学习。</p>
<p>在大语言模型微调的过程中，LoRA 冻结了预先训练好的模型权重，并将可训练的秩的分解矩阵注入到 Transformer 体系结构的每一层。例如，对于预训练的权重矩阵 W0，可以让其更新受到用低秩分解表示后者的约束：</p>
<ul>
<li>跟踪权重的变化而不是直接更新它们。</li>
<li>将大型权重矩阵分解为包含 “可训练参数” 的较小矩阵。</li>
</ul>
<p><img src="/../img/LLM-tuning-16.png" alt="1_N7Mrnoyvz9Qths1cC92WuQ"></p>
<h5 id="5-2-3-LoRA微调的优势"><a href="#5-2-3-LoRA微调的优势" class="headerlink" title="5.2.3 LoRA微调的优势"></a>5.2.3 LoRA 微调的优势</h5><ol>
<li>可训练参数显着减少，从而实现更快、更高效的微调。</li>
<li>保留原始的预训练权重，允许针对不同任务使用多个轻量级模型。</li>
<li>与其他参数高效方法兼容，可实现进一步优化。</li>
<li>在许多情况下，其性能可与完全微调的模型相媲美。</li>
<li>没有额外的推理延迟，因为适配器权重可以与基本模型合并。</li>
</ol>
<h5 id="5-2-4-QLoRA微调"><a href="#5-2-4-QLoRA微调" class="headerlink" title="5.2.4 QLoRA微调"></a>5.2.4 QLoRA 微调</h5><p>QLoRa 通过量化可训练参数，用更少的位数表示它们，使 LORA 更进一步。这进一步减小了模型大小，有可能实现在内存和计算资源有限的设备上的部署。</p>
<h4 id="5-3-Freeze微调"><a href="#5-3-Freeze微调" class="headerlink" title="5.3 Freeze微调"></a>5.3 Freeze 微调</h4><p>Freeze 方法，即参数冻结，对原始模型部分参数进行冻结操作，仅训练部分参数，以达到在单卡或不进行 TP 或 PP 操作，就可以对大模型进行训练。在语言模型模型微调中，Freeze 微调方法仅微调 Transformer 后几层的全连接层参数，而冻结其它所有参数。</p>
<h4 id="5-4-GaLore微调"><a href="#5-4-GaLore微调" class="headerlink" title="5.4 GaLore微调"></a>5.4 GaLore 微调</h4><p>GaLore 是一种允许全参数学习的训练策略，但比常见的低秩自适应方法（例如 LoRA）更节省内存。GaLore 关键思想是利用权重矩阵 W 的梯度缓慢变化的低秩结构，而不是试图将权重矩阵直接近似为低秩形式。使得在消费级 GPU 上训练大型语言模型成为可能。这一策略为未来的大模型训练提供了重要的技术支持，具有广泛的应用前景。</p>
<p><img src="/../img/LLM-tuning-18.png" alt="198"></p>
<h3 id="6-代理-Agent"><a href="#6-代理-Agent" class="headerlink" title="6. 代理(Agent)"></a>6. 代理 (Agent)</h3><p>增加辅助代理是指在主要模型之外引入其他小模型或组件，这些辅助代理可以帮助主模型更好地完成任务。辅助代理可能负责不同的子任务，如噪声过滤、特征增强、错误检测等，它们的工作成果可以作为额外的信息提供给主模型，从而帮助主模型做出更准确的决策。这种方法可以提高系统的鲁棒性和灵活性，使其在复杂环境中表现出色。</p>
<p><img src="/../img/LLM-tuning-26.png" alt="image-20240905142400736"></p>
<hr>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><h3 id="1-模型深度与宽度的平衡"><a href="#1-模型深度与宽度的平衡" class="headerlink" title="1. 模型深度与宽度的平衡"></a>1. 模型深度与宽度的平衡</h3><p>在语言模型中，增加模型的深度可以使其捕获更复杂的语言结构，而增加宽度则可以提供更多的表达能力。然而，深度和宽度的增加都会带来更高的计算成本和潜在的过拟合风险。可以通过实验来确定最优的层数和每层的隐藏单元数，找到一个既能提高模型性能又不会过度增加训练时间的平衡点。<img src="/../img/LLM-tuning-19.png" alt="image-20240905134548220"></p>
<h3 id="2-残差连接"><a href="#2-残差连接" class="headerlink" title="2. 残差连接"></a>2. 残差连接</h3><p>在语言模型中引入残差连接可以帮助模型更好地学习长期依赖关系，并减轻梯度消失的问题。通过让信息流绕过一层或多层，模型可以更容易地学习到输入与输出之间的映射关系。</p>
<p><img src="/../img/LLM-tuning-20.png" alt="ResBlock"></p>
<h3 id="3-注意力机制"><a href="#3-注意力机制" class="headerlink" title="3. 注意力机制"></a>3. 注意力机制</h3><p>在语言模型中使用自注意力机制，可以让模型学习到输入序列中不同位置之间的相关性。这种机制尤其适合处理变长的输入序列，因为它能够动态地为不同位置的词分配不同的权重。</p>
<h3 id="4-多头注意力"><a href="#4-多头注意力" class="headerlink" title="4. 多头注意力"></a>4. 多头注意力</h3><p>通过将注意力机制分成多个头，每个头可以独立地关注输入的不同方面。这种方式能够增强模型对输入的多样性和复杂性的理解，从而提升整体性能。</p>
<h3 id="5-Transformer架构"><a href="#5-Transformer架构" class="headerlink" title="5. Transformer架构"></a>5. Transformer 架构</h3><p>即便不采用卷积，也可以使用基于注意力机制的 Transformer 架构来构建语言模型。这种架构通过完全依赖自注意力机制来处理序列数据，避免了传统 RNN 中顺序依赖的问题，并且能够并行化训练过程，加快训练速度。即便不采用卷积，也可以使用基于注意力机制的 Transformer 架构来构建语言模型。这种架构通过完全依赖自注意力机制来处理序列数据，避免了传统 RNN 中顺序依赖的问题，并且能够并行化训练过程，加快训练速度。</p>
<p><img src="/../img/LLM-tuning-21.png" alt="image-20240905135346186"></p>
<h3 id="6-正则化技术"><a href="#6-正则化技术" class="headerlink" title="6. 正则化技术"></a>6. 正则化技术</h3><p>为了防止过拟合，可以在模型中加入正则化技术，比如 Dropout，它通过在训练过程中随机关闭一部分神经元来提高模型的鲁棒性。此外，还可以使用权重衰减等其他形式的正则化来约束模型复杂度。</p>
<p style="text-align:center">应用于标准神经网络的 Dropout</p>

<p><img src="/LLM-tuning-22.png" alt="image-20240905135513919"></p>
<h3 id="7-混合专家-Mixture-of-Experts-MoE"><a href="#7-混合专家-Mixture-of-Experts-MoE" class="headerlink" title="7 混合专家(Mixture of Experts, MoE)"></a>7 混合专家 (Mixture of Experts, MoE)</h3><p>在语言模型中，MoE 架构允许模型根据输入选择不同的专家来处理，从而在不增加太多参数的情况下提高模型的容量和表现力。这种方式特别适用于需要处理多种类型数据的语言任务。</p>
<p style="text-align:center">MoE 架构的核心组件</p>

<p><img src="/LLM-tuning-23.png" alt="image-20240905135601961"></p>
<hr>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>可怜可怜作者吧！</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.png" alt="潘秉宏 微信">
        <span>微信</span>
      </div>
      <div>
        <img src="/images/alipay.png" alt="潘秉宏 支付宝">
        <span>支付宝</span>
      </div>

  </div>
</div>

          <div class="followme">
  <span>欢迎加我微信~</span>

  <div class="social-list">

      <div class="social-item">
          <span class="social-link">
            <span class="icon">
              <i class="fab fa-weixin"></i>
            </span>

            <span class="label">WeChat</span>
          </span>

          <img class="social-item-img" src="/images/Weixin.jpg">
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/llm/" rel="tag"># llm</a>
              <a href="/tags/train/" rel="tag"># train</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/08/29/%E5%89%8D%E7%AB%AF/20240829_%E4%BD%BF%E7%94%A8html%E5%88%9B%E5%BB%BA%E6%B3%A8%E5%86%8C%E8%A1%A8%E5%8D%95/" rel="prev" title="使用 html 创建注册表单">
                  <i class="fa fa-angle-left"></i> 使用 html 创建注册表单
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2021 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">潘秉宏</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">27k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:39</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div><script type="module">import * as THREE from "/lib/three.js"; window.THREE = THREE;</script><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/panpan02222" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  <script src="/js/third-party/pace.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
