<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/ghost-32-32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/ghost-16-16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="深入学习 Tranformer 模型结构，本来依赖 GPT4o, 自己快复现完了，结果在封装 Decoder 层的时候，发生了 mask 维度匹配不对的问题，查了很多资料最终还是没解决，后期再搞吧.">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer">
<meta property="og:url" content="http://example.com/2024/09/26/AI/20240926_%E5%9F%BA%E4%BA%8EPytorch%E6%9E%B6%E6%9E%84%E6%89%8B%E6%92%95Transformer/index.html">
<meta property="og:site_name" content="潘秉宏的博客">
<meta property="og:description" content="深入学习 Tranformer 模型结构，本来依赖 GPT4o, 自己快复现完了，结果在封装 Decoder 层的时候，发生了 mask 维度匹配不对的问题，查了很多资料最终还是没解决，后期再搞吧.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/transformer-1.jpg">
<meta property="og:image" content="http://example.com/img/transformer-2.svg">
<meta property="og:image" content="http://example.com/img/transformer-6.jpg">
<meta property="og:image" content="http://example.com/img/transformer-7.jpg">
<meta property="og:image" content="http://example.com/img/transformer-8.jpg">
<meta property="og:image" content="http://example.com/img/transformer-3.png">
<meta property="og:image" content="http://example.com/img/transformer-5.jpg">
<meta property="og:image" content="http://example.com/img/transformer-4.jpg">
<meta property="article:published_time" content="2024-09-26T01:25:06.000Z">
<meta property="article:modified_time" content="2024-09-30T07:01:41.094Z">
<meta property="article:author" content="潘秉宏">
<meta property="article:tag" content="python">
<meta property="article:tag" content="LLMs">
<meta property="article:tag" content="transformer">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/transformer-1.jpg">


<link rel="canonical" href="http://example.com/2024/09/26/AI/20240926_%E5%9F%BA%E4%BA%8EPytorch%E6%9E%B6%E6%9E%84%E6%89%8B%E6%92%95Transformer/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2024/09/26/AI/20240926_%E5%9F%BA%E4%BA%8EPytorch%E6%9E%B6%E6%9E%84%E6%89%8B%E6%92%95Transformer/","path":"2024/09/26/AI/20240926_基于Pytorch架构手撕Transformer/","title":"Transformer"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Transformer | 潘秉宏的博客</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>
<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">潘秉宏的博客</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer-%E7%9A%84%E8%83%8C%E6%99%AF"><span class="nav-number">1.</span> <span class="nav-text">Transformer 的背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">2.</span> <span class="nav-text">注意力机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Scaled-Dot-product-Attention"><span class="nav-number">2.1.</span> <span class="nav-text">Scaled Dot-product Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E6%95%B4%E5%90%88-Scaled-Dot-product-Attention"><span class="nav-number">2.2.</span> <span class="nav-text">代码整合 (Scaled Dot-product Attention)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-head-Attention"><span class="nav-number">2.3.</span> <span class="nav-text">Multi-head Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E6%95%B4%E5%90%88-Multi-head-Attention"><span class="nav-number">2.4.</span> <span class="nav-text">代码整合 (Multi-head Attention)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">3.</span> <span class="nav-text">前馈神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Feed-Forward-Layer"><span class="nav-number">3.1.</span> <span class="nav-text">The Feed-Forward Layer</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96-%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5"><span class="nav-number">4.</span> <span class="nav-text">层归一化 &amp; 残差连接</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Transformer-Encoder-Layer"><span class="nav-number">4.1.</span> <span class="nav-text">Transformer Encoder Layer</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%9D%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="nav-number">5.</span> <span class="nav-text">绝对位置编码</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Transformer-Encoder"><span class="nav-number">5.1.</span> <span class="nav-text">Transformer Encoder</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="nav-number">6.</span> <span class="nav-text">完整代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%9D%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-1"><span class="nav-number">7.</span> <span class="nav-text">绝对位置编码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">8.</span> <span class="nav-text">多头注意力机制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-1"><span class="nav-number">9.</span> <span class="nav-text">前馈神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96-%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5-1"><span class="nav-number">10.</span> <span class="nav-text">层归一化 &amp; 残差连接</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Layer-Normalization"><span class="nav-number">10.1.</span> <span class="nav-text">Layer Normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Residual-Connections"><span class="nav-number">10.2.</span> <span class="nav-text">Residual Connections</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8"><span class="nav-number">11.</span> <span class="nav-text">编码器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder-Layer"><span class="nav-number">11.1.</span> <span class="nav-text">Encoder Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder"><span class="nav-number">11.2.</span> <span class="nav-text">Encoder</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="nav-number">12.</span> <span class="nav-text">解码器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Decoder-Layer"><span class="nav-number">12.1.</span> <span class="nav-text">Decoder_Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decoder"><span class="nav-number">12.2.</span> <span class="nav-text">Decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#References"><span class="nav-number">12.3.</span> <span class="nav-text">References</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="潘秉宏"
      src="/images/avatar.gif">
  <!--
  <p class="site-author-name" itemprop="name">潘秉宏</p>
  -->
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">43</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">36</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/panpan02222" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;panpan02222" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://19909442097@163.com/" title="E-Mail → https:&#x2F;&#x2F;19909442097@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/09/26/AI/20240926_%E5%9F%BA%E4%BA%8EPytorch%E6%9E%B6%E6%9E%84%E6%89%8B%E6%92%95Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="潘秉宏">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘秉宏的博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Transformer | 潘秉宏的博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Transformer
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-09-26 09:25:06" itemprop="dateCreated datePublished" datetime="2024-09-26T09:25:06+08:00">2024-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-09-30 15:01:41" itemprop="dateModified" datetime="2024-09-30T15:01:41+08:00">2024-09-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>29 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>深入学习 Tranformer 模型结构，本来依赖 GPT4o, 自己快复现完了，结果在封装 Decoder 层的时候，发生了 mask 维度匹配不对的问题，查了很多资料最终还是没解决，后期再搞吧. </p>
<p><img src="/../../img/transformer-1.jpg"></p>
<span id="more"></span>

<h2 id="Transformer-的背景"><a href="#Transformer-的背景" class="headerlink" title="Transformer 的背景"></a>Transformer 的背景</h2><ul>
<li><p>什么是 <strong>Transformer</strong> ? </p>
<p><strong>深度学习模型</strong> , 主要用于处理序列数据 (Sequence) 数据，Transformer 通过引入自注意力机制 (Self-Attention) , 比传统的 RNN, LSTM 牛逼很多，提高模型的训练速度和性能.</p>
</li>
<li><p>Transformer 是谁提出的？</p>
<p><strong>Google</strong> 团队的 Vaswani 等人</p>
</li>
<li><p>Transformer 解决了什么问题？</p>
<p><strong>长距离依赖问题</strong> : </p>
<p>通过自注意力机制，使得模型在处理 Sequence 中的每个位置时，都能考虑到 Sequence 中其他的位置 (更加考虑全局)</p>
<p><strong>并行计算问题</strong> : </p>
<p> 通过自注意力机制，使得模型能够并行处理输入序列中的每个位置</p>
</li>
<li><p>Transformer 核心组件有哪些？</p>
<ul>
<li>整体结构 : <strong>编码器 (Encoder) &amp; 解码器 (Decoder)</strong></li>
</ul>
<p><img src="/../../img/transformer-2.svg"></p>
<blockquote>
<p><strong>拓展 :</strong> </p>
<p>这两个模块可以根据任务的需求而单独使用：</p>
<ul>
<li><strong>Encoder ：</strong>适用于只需要理解输入语义的任务，例如句子分类、命名实体识别</li>
<li><strong> Decoder ：</strong>适用于生成式任务，例如文本生成</li>
<li><strong> Encoder-Decoder(Seq2Seq)</strong> : 适用于需要基于输入的生成式任务，例如翻译、摘要</li>
</ul>
</blockquote>
<ul>
<li>核心组成部分 :</li>
</ul>
<table>
<thead>
<tr>
<th align="center"> 多头自注意力机制 (Multi-Head Self Attention)</th>
<th align="center"> 前馈神经网络 (Feed-Forword Neural Network)</th>
<th align="center"> 位置编码 (Positional Encoding)</th>
<th align="center"> 残差连接与层归一化 (Residual Connections &amp; Layer Normalization)</th>
</tr>
</thead>
</table>
</li>
</ul>
<hr>
<h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><ul>
<li><p>核心公式 :<br>  <img src="/../../img/transformer-6.jpg" alt="image-20240930145549020"></p>
<p>  <img src="/../../img/transformer-7.jpg" alt="image-20240930145912200"></p>
<p>  <img src="/../../img/transformer-8.jpg" alt="image-20240930150003747"></p>
</li>
<li><p>论文原图 :</p>
<p><img src="/../../img/transformer-3.png"></p>
</li>
</ul>
<h3 id="Scaled-Dot-product-Attention"><a href="#Scaled-Dot-product-Attention" class="headerlink" title="Scaled Dot-product Attention"></a>Scaled Dot-product Attention</h3><p><img src="/../../img/transformer-5.jpg"></p>
<ul>
<li><p>代码实现 : </p>
<ol>
<li><p>构建 token id 至 token embedding 的<strong>映射表</strong></p>
 <figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入必要的库</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  <span class="comment"># 导入PyTorch的nn模块，用于定义神经网络层</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoConfig  <span class="comment"># 导入自动配置类，用于获取预训练模型的配置</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer  <span class="comment"># 导入自动分词器类，用于文本的分词和编码</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定预训练的BERT模型</span></span><br><span class="line">cache_dir = <span class="string">'./pretrained_model'</span></span><br><span class="line">model_ckpt = <span class="string">"bert-base-uncased"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化分词器</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_ckpt, cache_dir=cache_dir)  <span class="comment"># 从预训练模型加载分词器</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备输入文本</span></span><br><span class="line">text = <span class="string">"hello world"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用分词器处理文本，返回特殊的tensor格式</span></span><br><span class="line">inputs = tokenizer(text, return_tensors=<span class="string">"pt"</span>, add_special_tokens=<span class="literal">False</span>)  <span class="comment"># 不添加特殊标记</span></span><br><span class="line"><span class="built_in">print</span>(inputs.input_ids)  <span class="comment"># 打印输入的token ids</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取模型的配置</span></span><br><span class="line">config = AutoConfig.from_pretrained(model_ckpt)  <span class="comment"># 从预训练模型加载配置</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个嵌入层，用于将token ids转换为词向量</span></span><br><span class="line"><span class="comment"># vocab_size是词汇表的大小，hidden_size是嵌入向量的维度</span></span><br><span class="line">token_emb = nn.Embedding(config.vocab_size, config.hidden_size)</span><br><span class="line"><span class="built_in">print</span>(token_emb)  <span class="comment"># 打印嵌入层</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将输入的token ids通过嵌入层转换为词向量</span></span><br><span class="line">inputs_embeds = token_emb(inputs.input_ids)  <span class="comment"># 调用嵌入层的forward方法</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">inputs_embeds.size()函数的作用为 : 返回一个元组，其中 1 表示批次大小为1（单个样本），seq_length 表示输入文本中的单词数量，embedding_dim 是BERT模型的隐藏层大小，对于bert-base-uncased模型，这个维度通常是768。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(inputs_embeds.size())  <span class="comment"># 打印词向量的尺寸</span></span><br></pre></td></tr></tbody></table></figure>

<p> 输出结果为 : </p>
 <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[7592, 2088]])</span><br><span class="line">Embedding(30522, 768)</span><br><span class="line">torch.Size([1, 2, 768])</span><br></pre></td></tr></tbody></table></figure>

<p> 该部分解释：首先需要将文本分词为词语 (token) 序列，然后将每一个词语转换为对应的词向量 (embedding)。BERT-base-uncased 模型对应的词表大小为 30522，每个词语的词向量维度为 768。Embedding 层把输入的词语序列映射到了尺寸为 [batch_size, seq_len, hidden_dim] 的张量。</p>
</li>
<li><p>创建 query、key、value, 最终获得<strong>注意力分数</strong></p>
 <figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  <span class="comment"># 导入PyTorch库，用于进行张量运算</span></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt  <span class="comment"># 导入math库中的sqrt函数，用于计算平方根</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设inputs_embeds是之前步骤中得到的词嵌入向量，这里同时作为查询（Q）、键（K）和值（V）</span></span><br><span class="line">Q = K = V = inputs_embeds  <span class="comment"># 这里的Q, K, V是相同的词嵌入向量，但在实际应用中它们可能来自不同的输入</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取键（K）的维度大小，即键的嵌入维度</span></span><br><span class="line">dim_k = K.size(-<span class="number">1</span>)  <span class="comment"># -1表示最后一个维度，这里是嵌入向量的维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算注意力分数</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">为了计算Q和K的点积，我们需要调整K的维度，以便它与Q的维度兼容。原始的K矩阵具有形状(batch_size, sent_len, emb_dim)，</span></span><br><span class="line"><span class="string">为了执行矩阵乘法，我们需要将K的第二个和第三个维度交换，这里我们可以使用转置操作transpose(1, 2)将K的维度变为(batch_size, emb_dim, sent_len)。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">现在，Q和K的形状都是(batch_size, sent_len, emb_dim)，我们可以执行矩阵乘法。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Q 的形状：(batch_size, sent_len, emb_dim)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">K 的转置的形状：(batch_size, emb_dim, sent_len)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">注意力分数的形状：(batch_size, sent_len, sent_len)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># 每个分数表示一个查询向量与所有键向量之间的相似度</span></span><br><span class="line">scores = torch.bmm(Q, K.transpose(<span class="number">1</span>, <span class="number">2</span>)) / sqrt(dim_k)  <span class="comment"># 计算得到的分数矩阵并缩放</span></span><br><span class="line"><span class="comment"># 这个矩阵中的每个元素表示一个查询向量与一个键向量之间的相似度分数。</span></span><br><span class="line"><span class="built_in">print</span>(socres)</span><br><span class="line"><span class="comment"># 打印注意力分数的形状</span></span><br><span class="line"><span class="built_in">print</span>(scores.size())  <span class="comment"># 应该输出(batch_size, sent_len, sent_len)</span></span><br><span class="line"><span class="comment"># 具体来说，scores 的每个元素 scores[b, i, j] 表示第 b 个批次中第 i 个查询向量与第 j 个键向量之间的相似度分数。</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<p> 输出结果为 : </p>
 <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[25.9001, -0.7132],</span><br><span class="line">         [-0.7132, 25.8847]]], grad_fn=&lt;DivBackward0&gt;)</span><br><span class="line">torch.Size([1, 2, 2])</span><br></pre></td></tr></tbody></table></figure>

<p>  Q,K 的序列长度都为 2，因此生成了一个 2 x 2 的注意力分数矩阵</p>
</li>
<li><p><strong>Softmax 标准化</strong>注意力权重</p>
 <figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F  <span class="comment"># 导入PyTorch的nn.functional模块，包含了许多神经网络操作的函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用softmax函数对注意力分数进行归一化，得到注意力权重</span></span><br><span class="line"><span class="comment"># softmax函数将查询向量与键向量的相似度分数转换成一个概率分布。</span></span><br><span class="line"><span class="comment"># 这里的dim=-1指定了softmax操作的维度，即沿着最后一个维度（sent_len）进行，这个维度表示的是键向量的索引。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">简单讲下，假设</span></span><br><span class="line"><span class="string">scores = [[[1.0, 2.0, 3.0],</span></span><br><span class="line"><span class="string">           [4.0, 5.0, 6.0],</span></span><br><span class="line"><span class="string">           [7.0, 8.0, 9.0]]]</span></span><br><span class="line"><span class="string">1.计算每个元素的指数值：</span></span><br><span class="line"><span class="string">exp_scores = [[[exp(1.0), exp(2.0), exp(3.0)],</span></span><br><span class="line"><span class="string">               [exp(4.0), exp(5.0), exp(6.0)],</span></span><br><span class="line"><span class="string">               [exp(7.0), exp(8.0), exp(9.0)]]]</span></span><br><span class="line"><span class="string">2.计算每个查询向量的指数值之和：</span></span><br><span class="line"><span class="string">sum_exp_scores = [[exp(1.0) + exp(2.0) + exp(3.0),</span></span><br><span class="line"><span class="string">                   exp(4.0) + exp(5.0) + exp(6.0),</span></span><br><span class="line"><span class="string">                   exp(7.0) + exp(8.0) + exp(9.0)]]</span></span><br><span class="line"><span class="string">3.计算 softmax 值：</span></span><br><span class="line"><span class="string">softmax_scores = [[[exp(1.0) / sum_exp_scores[0][0], exp(2.0) / sum_exp_scores[0][0], exp(3.0) / sum_exp_scores[0][0]],</span></span><br><span class="line"><span class="string">                    [exp(4.0) / sum_exp_scores[0][1], exp(5.0) / sum_exp_scores[0][1], exp(6.0) / sum_exp_scores[0][1]],</span></span><br><span class="line"><span class="string">                    [exp(7.0) / sum_exp_scores[0][2], exp(8.0) / sum_exp_scores[0][2], exp(9.0) / sum_exp_scores[0][2]]]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">至于为什么不能在查询向量上面做，主要是因为查询向量和键向量的作用不同。查询向量（Q）用于表示我们想要获取信息的请求，而键向量（K）用于表示与查询向量进行比较的键。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">weights = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(weights)  <span class="comment"># 打印权重</span></span><br><span class="line"><span class="comment"># 打印权重的和，dim=-1表示沿着最后一个维度（即每个词的权重和）进行求和</span></span><br><span class="line"><span class="comment"># 由于softmax函数的输出是概率分布，每个维度的和应该接近1（如果不是1，可能是由于浮点数精度问题）</span></span><br><span class="line"><span class="built_in">print</span>(weights.<span class="built_in">sum</span>(dim=-<span class="number">1</span>))  <span class="comment"># 打印每个查询词的权重和，理论上应该接近1</span></span><br></pre></td></tr></tbody></table></figure>

<p> 输出结果为 : </p>
 <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[1.0000e+00, 2.7667e-12],</span><br><span class="line">         [2.8098e-12, 1.0000e+00]]], grad_fn=&lt;SoftmaxBackward0&gt;)</span><br><span class="line">tensor([[1., 1.]], grad_fn=&lt;SumBackward1&gt;)</span><br></pre></td></tr></tbody></table></figure>

<p> 打印每个查询词的权重和，理论上应该接近 1</p>
</li>
<li><p>注意力权重与 value 相乘</p>
 <figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">attn_outputs = torch.bmm(weights, V)</span><br><span class="line"><span class="built_in">print</span>(attn_outputs)</span><br><span class="line"><span class="built_in">print</span>(attn_outputs.shape)</span><br></pre></td></tr></tbody></table></figure>

<p> 输出 : </p>
 <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[-0.1071, -1.8374, -0.2551,  ..., -0.2630,  0.3523,  0.1242],</span><br><span class="line">         [-0.8179,  0.8697, -1.4724,  ...,  0.6556,  0.3608,  0.0020]]],</span><br><span class="line">       grad_fn=&lt;BmmBackward0&gt;)</span><br><span class="line">torch.Size([1, 2, 768])</span><br></pre></td></tr></tbody></table></figure></li>
</ol>
</li>
</ul>
<h3 id="代码整合-Scaled-Dot-product-Attention"><a href="#代码整合-Scaled-Dot-product-Attention" class="headerlink" title="代码整合(Scaled Dot-product Attention)"></a>代码整合 (Scaled Dot-product Attention)</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  <span class="comment"># 导入PyTorch库</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F  <span class="comment"># 导入PyTorch的nn.functional模块，包含了许多神经网络操作的函数</span></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt  <span class="comment"># 导入math库中的sqrt函数，用于计算平方根</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义Scaled Dot-product Attention函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">scaled_dot_product_attention</span>(<span class="params">query, key, value, query_mask=<span class="literal">None</span>, key_mask=<span class="literal">None</span>, mask=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># 获取查询（query）的最后一个维度大小，即键（key）的维度</span></span><br><span class="line">    dim_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算查询和键的点积，并缩放，得到未归一化的注意力分数</span></span><br><span class="line">    scores = torch.bmm(query, key.transpose(<span class="number">1</span>, <span class="number">2</span>)) / sqrt(dim_k)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果提供了查询掩码（query_mask）和键掩码（key_mask），则计算掩码矩阵</span></span><br><span class="line">    <span class="keyword">if</span> query_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> key_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        mask = torch.bmm(query_mask.unsqueeze(-<span class="number">1</span>), key_mask.unsqueeze(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果没有提供掩码，则使用之前传入的掩码（如果有的话）</span></span><br><span class="line">        mask = mask</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果存在掩码，则将分数矩阵中与掩码对应位置为0的分数替换为负无穷</span></span><br><span class="line">    <span class="comment"># 这样在应用softmax时，这些位置的权重会接近于0</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="built_in">float</span>(<span class="string">"inf"</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用softmax函数对分数进行归一化，得到注意力权重</span></span><br><span class="line">    weights = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算加权后的输出，即将注意力权重与值（value）相乘</span></span><br><span class="line">    <span class="comment"># 这里的输出是经过注意力加权后的值向量，用于下游任务</span></span><br><span class="line">    <span class="keyword">return</span> torch.bmm(weights, value)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<h3 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h3><ul>
<li><p><strong>Multi-head Attention 作用</strong> :<br>首先通过线性映射将 Q,K,V 序列映射到特征空间，每一组线性投影后的向量表示称为一个头 (head)，然后在每组映射后的序列上再应用 Scaled Dot-product Attention, 最后将每个头的向量拼接.</p>
<p><img src="/../../img/transformer-4.jpg"></p>
</li>
</ul>
<p>	</p>
<h3 id="代码整合-Multi-head-Attention"><a href="#代码整合-Multi-head-Attention" class="headerlink" title="代码整合(Multi-head Attention)"></a>代码整合 (Multi-head Attention)</h3><ul>
<li><p>代码实现 : </p>
<ol>
<li><p>单头实现</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义AttentionHead类，继承自nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionHead</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 初始化函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_dim, head_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment"># 调用基类的初始化方法</span></span><br><span class="line">        <span class="comment"># 定义线性层，用于将输入的词嵌入向量转换为查询（q）、键（k）和值（v）向量</span></span><br><span class="line">        <span class="comment"># embed_dim是输入嵌入的维度，head_dim是每个头输出的维度</span></span><br><span class="line">        <span class="variable language_">self</span>.q = nn.Linear(embed_dim, head_dim)</span><br><span class="line">        <span class="variable language_">self</span>.k = nn.Linear(embed_dim, head_dim)</span><br><span class="line">        <span class="variable language_">self</span>.v = nn.Linear(embed_dim, head_dim)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, query_mask=<span class="literal">None</span>, key_mask=<span class="literal">None</span>, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 调用scaled_dot_product_attention函数，传入通过线性层转换后的查询、键和值</span></span><br><span class="line">        <span class="comment"># 同时传入可选的掩码参数</span></span><br><span class="line">        attn_outputs = scaled_dot_product_attention(</span><br><span class="line">            <span class="variable language_">self</span>.q(query),  <span class="comment"># 经过查询线性层转换的query</span></span><br><span class="line">            <span class="variable language_">self</span>.k(key),     <span class="comment"># 经过键线性层转换的key</span></span><br><span class="line">            <span class="variable language_">self</span>.v(value),   <span class="comment"># 经过值线性层转换的value</span></span><br><span class="line">            query_mask,      <span class="comment"># 查询掩码</span></span><br><span class="line">            key_mask,        <span class="comment"># 键掩码</span></span><br><span class="line">            mask             <span class="comment"># 已有的掩码（如果有的话）</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 返回注意力机制的输出</span></span><br><span class="line">        <span class="keyword">return</span> attn_outputs</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>多头实现</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义MultiHeadAttention类，继承自nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 初始化函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment"># 调用基类的初始化方法</span></span><br><span class="line">        <span class="comment"># 从配置中获取嵌入维度和注意力头的数量</span></span><br><span class="line">        embed_dim = config.hidden_size</span><br><span class="line">        num_heads = config.num_attention_heads</span><br><span class="line">        <span class="comment"># 计算每个头的维度大小</span></span><br><span class="line">        head_dim = embed_dim // num_heads</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建一个包含多个AttentionHead模块的列表</span></span><br><span class="line">        <span class="comment"># 每个头都使用相同的嵌入维度和头维度</span></span><br><span class="line">        <span class="variable language_">self</span>.heads = nn.ModuleList(</span><br><span class="line">            [AttentionHead(embed_dim, head_dim) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_heads)]</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义输出线性层，用于将多头注意力的输出合并</span></span><br><span class="line">        <span class="variable language_">self</span>.output_linear = nn.Linear(embed_dim, embed_dim)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, query_mask=<span class="literal">None</span>, key_mask=<span class="literal">None</span>, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 并行通过每个注意力头处理输入</span></span><br><span class="line">        <span class="comment"># 使用torch.cat将所有头的输出在最后一个维度上拼接起来</span></span><br><span class="line">        x = torch.cat([</span><br><span class="line">            h(query, key, value, query_mask, key_mask, mask) <span class="keyword">for</span> h <span class="keyword">in</span> <span class="variable language_">self</span>.heads</span><br><span class="line">        ], dim=-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 通过输出线性层处理拼接后的输出</span></span><br><span class="line">        x = <span class="variable language_">self</span>.output_linear(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 返回最终的输出</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure></li>
</ol>
</li>
<li><p>验证代码</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoConfig</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_ckpt = <span class="string">"bert-base-uncased"</span></span><br><span class="line">cache_dir = <span class="string">"./pretrained_path"</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_ckpt, cache_dir=cache_dir)</span><br><span class="line"></span><br><span class="line">text = <span class="string">"hello world"</span></span><br><span class="line">inputs = tokenizer(text, return_tensors=<span class="string">"pt"</span>, add_special_tokens=<span class="literal">False</span>)</span><br><span class="line">config = AutoConfig.from_pretrained(model_ckpt)</span><br><span class="line">token_emb = nn.Embedding(config.vocab_size, config.hidden_size)</span><br><span class="line">inputs_embeds = token_emb(inputs.input_ids)</span><br><span class="line"></span><br><span class="line">multihead_attn = MultiHeadAttention(config)</span><br><span class="line">query = key = value = inputs_embeds</span><br><span class="line">attn_output = multihead_attn(query, key, value)</span><br><span class="line"><span class="built_in">print</span>(attn_output.size()) <span class="comment">#torch.Size([1, 5, 768])</span></span><br></pre></td></tr></tbody></table></figure>

<p>输出 : </p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([1, 2, 768])</span><br></pre></td></tr></tbody></table></figure></li>
</ul>
<hr>
<h2 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h2><h3 id="The-Feed-Forward-Layer"><a href="#The-Feed-Forward-Layer" class="headerlink" title="The Feed-Forward Layer"></a>The Feed-Forward Layer</h3><p>没啥好写的，就是普通的全连接 + 激活函数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义FeedForward类，继承自nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 初始化函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment"># 调用基类的初始化方法</span></span><br><span class="line">        <span class="comment"># 定义第一个线性层，将输入的隐藏状态映射到中间维度</span></span><br><span class="line">        <span class="variable language_">self</span>.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)</span><br><span class="line">        <span class="comment"># 定义第二个线性层，将中间维度的表示映射回原始的隐藏状态维度</span></span><br><span class="line">        <span class="variable language_">self</span>.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)</span><br><span class="line">        <span class="comment"># 定义GELU激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.gelu = nn.GELU()</span><br><span class="line">        <span class="comment"># 定义Dropout层，用于防止过拟合</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 应用第一个线性层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.linear_1(x)</span><br><span class="line">        <span class="comment"># 应用GELU激活函数</span></span><br><span class="line">        x = <span class="variable language_">self</span>.gelu(x)</span><br><span class="line">        <span class="comment"># 应用第二个线性层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.linear_2(x)</span><br><span class="line">        <span class="comment"># 应用Dropout</span></span><br><span class="line">        x = <span class="variable language_">self</span>.dropout(x)</span><br><span class="line">        <span class="comment"># 返回最终的输出</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure>

<p>与上面构建的注意力机制串联测试 : </p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">feed_forward = FeedForward(config)</span><br><span class="line">ff_outputs = feed_forward(attn_output)</span><br><span class="line"><span class="built_in">print</span>(ff_outputs.size()) <span class="comment">#torch.Size([1, 2, 768])</span></span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="层归一化-残差连接"><a href="#层归一化-残差连接" class="headerlink" title="层归一化 &amp; 残差连接"></a>层归一化 &amp; 残差连接</h2><ul>
<li><p>层归一化模块需要包含在残差模块内，主要作用为 : <strong>将输入的一批向量，每一个都做标准化处理，处理为：均值为零，且有单位方差</strong></p>
</li>
<li><p>残差连接主要作用为 : <strong>是通过直接将输入绕过中间层的计算，帮助模型更容易训练深层网络，避免梯度消失问题并促进信息流动</strong></p>
</li>
</ul>
<h3 id="Transformer-Encoder-Layer"><a href="#Transformer-Encoder-Layer" class="headerlink" title="Transformer Encoder Layer"></a>Transformer Encoder Layer</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义TransformerEncoderLayer类，继承自nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 初始化函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment"># 调用基类的初始化方法</span></span><br><span class="line">        <span class="comment"># 定义第一个层归一化，用于注意力机制之前</span></span><br><span class="line">        <span class="variable language_">self</span>.layer_norm_1 = nn.LayerNorm(config.hidden_size)</span><br><span class="line">        <span class="comment"># 定义第二个层归一化，用于前馈网络之前</span></span><br><span class="line">        <span class="variable language_">self</span>.layer_norm_2 = nn.LayerNorm(config.hidden_size)</span><br><span class="line">        <span class="comment"># 定义多头注意力机制</span></span><br><span class="line">        <span class="variable language_">self</span>.attention = MultiHeadAttention(config)</span><br><span class="line">        <span class="comment"># 定义前馈神经网络</span></span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = FeedForward(config)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 应用第一个层归一化</span></span><br><span class="line">        hidden_state = <span class="variable language_">self</span>.layer_norm_1(x)</span><br><span class="line">        <span class="comment"># 应用注意力机制，并将结果与输入进行残差连接</span></span><br><span class="line">        <span class="comment"># 注意力机制的输出将与输入x相加，得到更新后的x</span></span><br><span class="line">        x = x + <span class="variable language_">self</span>.attention(hidden_state, hidden_state, hidden_state, mask=mask)</span><br><span class="line">        <span class="comment"># 应用第二个层归一化</span></span><br><span class="line">        <span class="comment"># 注意这里的self.layer_norm_2(x)实际上是对更新后的x进行归一化</span></span><br><span class="line">        hidden_state = <span class="variable language_">self</span>.layer_norm_2(x)</span><br><span class="line">        <span class="comment"># 应用前馈网络，并将结果与更新后的x进行残差连接</span></span><br><span class="line">        x = x + <span class="variable language_">self</span>.feed_forward(hidden_state)</span><br><span class="line">        <span class="comment"># 返回最终的输出x</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure>

<p>代码验证 : </p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">encoder_layer = TransformerEncoderLayer(config)</span><br><span class="line"><span class="built_in">print</span>(inputs_embeds.shape)</span><br><span class="line"><span class="built_in">print</span>(encoder_layer(inputs_embeds).size())</span><br><span class="line"><span class="comment">#torch.Size([1, 5, 768])</span></span><br><span class="line"><span class="comment">#torch.Size([1, 5, 768])</span></span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="绝对位置编码"><a href="#绝对位置编码" class="headerlink" title="绝对位置编码"></a>绝对位置编码</h2><p>注意力机制无法捕获词语之间的位置信息，因此 Transformer 模型还使用 Positional Embeddings 添加了词语的位置信息。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, LongTensor, arange</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义Embeddings类，继承自nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 初始化函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment"># 调用基类的初始化方法</span></span><br><span class="line">        <span class="comment"># 定义词嵌入层，将词ID映射到词向量</span></span><br><span class="line">        <span class="variable language_">self</span>.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)</span><br><span class="line">        <span class="comment"># 定义位置嵌入层，为序列中的每个位置生成一个唯一的位置向量</span></span><br><span class="line">        <span class="variable language_">self</span>.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)</span><br><span class="line">        <span class="comment"># 定义层归一化，用于稳定训练过程</span></span><br><span class="line">        <span class="variable language_">self</span>.layer_norm = nn.LayerNorm(config.hidden_size, eps=<span class="number">1e-12</span>)</span><br><span class="line">        <span class="comment"># 定义Dropout层，用于防止过拟合</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids</span>):</span><br><span class="line">        <span class="comment"># 根据输入序列的长度创建位置ID</span></span><br><span class="line">        seq_length = input_ids.size(<span class="number">1</span>)  <span class="comment"># 获取序列长度</span></span><br><span class="line">        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(<span class="number">0</span>)  <span class="comment"># 创建位置ID序列</span></span><br><span class="line">        <span class="comment"># 创建词嵌入和位置嵌入</span></span><br><span class="line">        token_embeddings = <span class="variable language_">self</span>.token_embeddings(input_ids)  <span class="comment"># 通过词嵌入层获取词嵌入</span></span><br><span class="line">        position_embeddings = <span class="variable language_">self</span>.position_embeddings(position_ids)  <span class="comment"># 通过位置嵌入层获取位置嵌入</span></span><br><span class="line">        <span class="comment"># 将词嵌入和位置嵌入相加，得到最终的嵌入表示</span></span><br><span class="line">        embeddings = token_embeddings + position_embeddings</span><br><span class="line">        <span class="comment"># 应用层归一化</span></span><br><span class="line">        embeddings = <span class="variable language_">self</span>.layer_norm(embeddings)</span><br><span class="line">        <span class="comment"># 应用Dropout</span></span><br><span class="line">        embeddings = <span class="variable language_">self</span>.dropout(embeddings)</span><br><span class="line">        <span class="comment"># 返回最终的嵌入表示</span></span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Embeddings层的实例，并使用config配置</span></span><br><span class="line">embedding_layer = Embeddings(config)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用embedding_layer处理输入的词ID，并打印输出的大小</span></span><br><span class="line"><span class="comment"># 这里假设inputs.input_ids是之前通过tokenizer得到的词ID序列</span></span><br><span class="line"><span class="built_in">print</span>(embedding_layer(inputs.input_ids).size()) <span class="comment">#torch.Size([1, 5, 768])</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="Transformer-Encoder"><a href="#Transformer-Encoder" class="headerlink" title="Transformer Encoder"></a>Transformer Encoder</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义TransformerEncoder类，继承自nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoder</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 初始化函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment"># 调用基类的初始化方法</span></span><br><span class="line">        <span class="comment"># 创建嵌入层实例，用于将输入的词ID转换为嵌入向量</span></span><br><span class="line">        <span class="variable language_">self</span>.embeddings = Embeddings(config)</span><br><span class="line">        <span class="comment"># 创建一个包含多个Transformer编码器层的列表</span></span><br><span class="line">        <span class="comment"># num_hidden_layers表示编码器中隐藏层的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.layers = nn.ModuleList([TransformerEncoderLayer(config) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(config.num_hidden_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 首先通过嵌入层处理输入x</span></span><br><span class="line">        x = <span class="variable language_">self</span>.embeddings(x)</span><br><span class="line">        <span class="comment"># 然后依次通过每个编码器层</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            <span class="comment"># 将当前层的输出作为下一层的输入，并传递掩码（如果有的话）</span></span><br><span class="line">            x = layer(x, mask=mask)</span><br><span class="line">        <span class="comment"># 返回最终的编码器输出</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure>

<p>测试代码 : </p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">encoder = TransformerEncoder(config)</span><br><span class="line"><span class="built_in">print</span>(encoder(inputs.input_ids).size())  <span class="comment">#torch.Size([1, 5, 768])</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionHead</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_dim, head_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.q = nn.Linear(embed_dim, head_dim)</span><br><span class="line">        <span class="variable language_">self</span>.k = nn.Linear(embed_dim, head_dim)</span><br><span class="line">        <span class="variable language_">self</span>.v = nn.Linear(embed_dim, head_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        query, key, value = <span class="variable language_">self</span>.q(query), <span class="variable language_">self</span>.k(key), <span class="variable language_">self</span>.v(value)</span><br><span class="line">        scores = torch.bmm(query, key.transpose(<span class="number">1</span>, <span class="number">2</span>)) / sqrt(query.size(-<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="built_in">float</span>(<span class="string">"inf"</span>))</span><br><span class="line">        weights = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(weights, value)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        embed_dim = config.hidden_size</span><br><span class="line">        num_heads = config.num_attention_heads</span><br><span class="line">        head_dim = embed_dim // num_heads</span><br><span class="line">        <span class="variable language_">self</span>.heads = nn.ModuleList(</span><br><span class="line">            [AttentionHead(embed_dim, head_dim) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_heads)]</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.output_linear = nn.Linear(embed_dim, embed_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span>, query_mask=<span class="literal">None</span>, key_mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> query_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> key_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mask = torch.bmm(query_mask.unsqueeze(-<span class="number">1</span>), key_mask.unsqueeze(<span class="number">1</span>))</span><br><span class="line">        x = torch.cat([h(query, key, value, mask) <span class="keyword">for</span> h <span class="keyword">in</span> <span class="variable language_">self</span>.heads], dim=-<span class="number">1</span>)</span><br><span class="line">        x = <span class="variable language_">self</span>.output_linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)</span><br><span class="line">        <span class="variable language_">self</span>.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.gelu = nn.GELU()</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.linear_1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.gelu(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.linear_2(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.dropout(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layer_norm_1 = nn.LayerNorm(config.hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.layer_norm_2 = nn.LayerNorm(config.hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.attention = MultiHeadAttention(config)</span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = FeedForward(config)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># Apply layer normalization and then copy input into query, key, value</span></span><br><span class="line">        hidden_state = <span class="variable language_">self</span>.layer_norm_1(x)</span><br><span class="line">        <span class="comment"># Apply attention with a skip connection</span></span><br><span class="line">        x = x + <span class="variable language_">self</span>.attention(hidden_state, hidden_state, hidden_state, mask=mask)</span><br><span class="line">        <span class="comment"># Apply feed-forward layer with a skip connection</span></span><br><span class="line">        x = x + <span class="variable language_">self</span>.feed_forward(<span class="variable language_">self</span>.layer_norm_2(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.token_embeddings = nn.Embedding(config.vocab_size,</span><br><span class="line">                                             config.hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.position_embeddings = nn.Embedding(config.max_position_embeddings,</span><br><span class="line">                                                config.hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.layer_norm = nn.LayerNorm(config.hidden_size, eps=<span class="number">1e-12</span>)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids</span>):</span><br><span class="line">        <span class="comment"># Create position IDs for input sequence</span></span><br><span class="line">        seq_length = input_ids.size(<span class="number">1</span>)</span><br><span class="line">        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># Create token and position embeddings</span></span><br><span class="line">        token_embeddings = <span class="variable language_">self</span>.token_embeddings(input_ids)</span><br><span class="line">        position_embeddings = <span class="variable language_">self</span>.position_embeddings(position_ids)</span><br><span class="line">        <span class="comment"># Combine token and position embeddings</span></span><br><span class="line">        embeddings = token_embeddings + position_embeddings</span><br><span class="line">        embeddings = <span class="variable language_">self</span>.layer_norm(embeddings)</span><br><span class="line">        embeddings = <span class="variable language_">self</span>.dropout(embeddings)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.embeddings = Embeddings(config)</span><br><span class="line">        <span class="variable language_">self</span>.layers = nn.ModuleList(</span><br><span class="line">            [TransformerEncoderLayer(config) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(config.num_hidden_layers)]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.embeddings(x)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">from</span> transformers <span class="keyword">import</span> AutoConfig</span><br><span class="line">    <span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">    model_ckpt = <span class="string">"bert-base-uncased"</span>\</span><br><span class="line">    cache_dir = <span class="string">'./pretrained_model'</span>\</span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(model_ckpt, cache_dir=cache_dir)</span><br><span class="line">    config = AutoConfig.from_pretrained(model_ckpt)</span><br><span class="line"></span><br><span class="line">    text = <span class="string">"hello world"</span></span><br><span class="line">    inputs = tokenizer(text, return_tensors=<span class="string">"pt"</span>, add_special_tokens=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    encoder = TransformerEncoder(config)</span><br><span class="line">    <span class="built_in">print</span>(encoder(inputs.input_ids).size())</span><br></pre></td></tr></tbody></table></figure>

<hr>
<blockquote>
<p>[!IMPORTANT]</p>
<p>到此为止本篇博客，结束，以下部分是本人自己瞎写的，最终在 Decoder 部分报错，没有解决的代码。可以说以下部分其实是原文章，无奈啊，本人还是太菜了，解决不了报错，不然….</p>
</blockquote>
<h2 id="绝对位置编码-1"><a href="#绝对位置编码-1" class="headerlink" title="绝对位置编码"></a>绝对位置编码</h2><p>使用 Pytorch 实现 Transformer 中的绝对位置编码底层代码</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- encoding: utf-8 -*-</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">@File    :   Positional_Encoding.py</span></span><br><span class="line"><span class="string">@Time    :   2024/09/26 11:23:36</span></span><br><span class="line"><span class="string">@Author  :   pan binghong </span></span><br><span class="line"><span class="string">@Email   :   19909442097@163.com</span></span><br><span class="line"><span class="string">@description   :   Transformer中的绝对位置编码底层代码实现</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">file_path = os.path.abspath(__file__)</span><br><span class="line"></span><br><span class="line"><span class="built_in">dir</span> = os.path.dirname(file_path)</span><br><span class="line"></span><br><span class="line">os.chdir(<span class="built_in">dir</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="string">''''</span></span><br><span class="line"><span class="string">        :param d_model: 模型的维度</span></span><br><span class="line"><span class="string">        :param max_len: 序列的最大长度</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建一个形状为 (max_len, d_model) 的矩阵, 用于存储位置编码</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建一个形状为 (max_len, 1) 的矩阵, 用于存储位置信息, 保存索引值 e.g.[0, 1, 2, ... , max_len-1]</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这段代码计算位置编码中的两个频率。具体来说，它生成一个从0到d_model（不包括d_model）的偶数序列，</span></span><br><span class="line">        <span class="comment"># 然后将这些偶数转换为浮点数，并乘以一个常数因子 (-math.log(10000.0) / d_model)。</span></span><br><span class="line">        <span class="comment"># 这个常数因子是通过对10000取自然对数并除以d_model得到的。</span></span><br><span class="line">        <span class="comment"># 最后，通过torch.exp函数计算这些值的指数，得到最终的div_term。</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用正弦函数 得到偶数索引位置编码</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用余弦函数 得到奇数索引位置编码</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 增加一个 batch 维度, 使其能够与输入一起使用</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将位置编码矩阵注册为一个参数, 并将其添加到模型参数列表中</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">''''</span></span><br><span class="line"><span class="string">        :param x: 输入的序列张量, shape为: &lt;batch_size, seq_len, d_model&gt;</span></span><br><span class="line"><span class="string">        :return: 输出的序列张量, shape为: &lt;batch_size, seq_len, d_model&gt;</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = x + <span class="variable language_">self</span>.pe[:, :x.size(<span class="number">1</span>), :]</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    d_model = <span class="number">512</span></span><br><span class="line">    max_len = <span class="number">2048</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化位置编码</span></span><br><span class="line">    pos_encoder = PositionalEncoding(d_model, max_len)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化 tokenizer (这里以 Bert 为例)</span></span><br><span class="line">    tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-uncased'</span>, cache_dir=<span class="string">'./cache'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用输入"hello world"</span></span><br><span class="line">    input_text = <span class="string">"hello world"</span></span><br><span class="line">    input_ids = torch.tensor([tokenizer.encode(input_text, add_special_tokens=<span class="literal">True</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建一个形状为 (1, seq_len, d_model) 的零矩阵</span></span><br><span class="line">    x = torch.zeros(<span class="number">1</span>, input_ids.size(<span class="number">1</span>), d_model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 应用位置编码</span></span><br><span class="line">    output = pos_encoder(x)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Input Text:"</span>, input_text)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Input IDs Shape:"</span>, input_ids.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Output Shape:"</span>, output.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Output:"</span>, output)</span><br></pre></td></tr></tbody></table></figure>

<p>以上案例为：当输入 <code>hello world</code>, 经过编码后输出其对应的信息</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Input Text: hello world</span><br><span class="line">Input IDs Shape: torch.Size([1, 4])</span><br><span class="line">Output Shape: torch.Size([1, 4, 512])</span><br><span class="line">Output: tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,</span><br><span class="line">           0.0000e+00,  1.0000e+00],</span><br><span class="line">         [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,</span><br><span class="line">           1.0366e-04,  1.0000e+00],</span><br><span class="line">         [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,</span><br><span class="line">           2.0733e-04,  1.0000e+00],</span><br><span class="line">         [ 1.4112e-01, -9.8999e-01,  2.4509e-01,  ...,  1.0000e+00,</span><br><span class="line">           3.1099e-04,  1.0000e+00]]])</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h2><p>使用 Pytorch 实现<strong>多头注意力机制</strong>的底层代码，含例子.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- encoding: utf-8 -*-</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">@File    :   MultiHeadAttention.py</span></span><br><span class="line"><span class="string">@Time    :   2024/09/27 09:50:43</span></span><br><span class="line"><span class="string">@Author  :   pan binghong </span></span><br><span class="line"><span class="string">@Email   :   19909442097@163.com</span></span><br><span class="line"><span class="string">@description   :   </span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取当前文件的绝对路径</span></span><br><span class="line">file_path = os.path.abspath(__file__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取当前文件所在的目录路径</span></span><br><span class="line"><span class="built_in">dir</span> = os.path.dirname(file_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将当前工作目录更改为当前文件所在的目录</span></span><br><span class="line">os.chdir(<span class="built_in">dir</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_size, heads</span>):</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        初始化多头注意力机制</span></span><br><span class="line"><span class="string">        :param embed_size: 输入嵌入向量的维度</span></span><br><span class="line"><span class="string">        :param num_heads: 多头注意力机制的头数</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> embed_size % heads == <span class="number">0</span>, <span class="string">"嵌入维度必须能被头数整除"</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.embed_size = embed_size</span><br><span class="line">        <span class="variable language_">self</span>.heads = heads</span><br><span class="line">        <span class="comment"># 每个头的维度</span></span><br><span class="line">        <span class="variable language_">self</span>.heads_dim = embed_size // heads</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.value = nn.Linear(<span class="variable language_">self</span>.heads_dim, <span class="variable language_">self</span>.heads_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.key = nn.Linear(<span class="variable language_">self</span>.heads_dim, <span class="variable language_">self</span>.heads_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.query = nn.Linear(<span class="variable language_">self</span>.heads_dim, <span class="variable language_">self</span>.heads_dim, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.fc_out = nn.Linear(heads * <span class="variable language_">self</span>.heads_dim, embed_size)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, keys, values, mask</span>):</span><br><span class="line">        N = query.shape[<span class="number">0</span>]</span><br><span class="line">        value_len, key_len, query_len = values.shape[<span class="number">1</span>], keys.shape[<span class="number">1</span>], query.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将输入的数据划分为多个头, 先调整shape为 (N, value_len, heads, heads_dim)</span></span><br><span class="line">        values = values.reshape(N, value_len, <span class="variable language_">self</span>.heads, <span class="variable language_">self</span>.heads_dim)</span><br><span class="line">        keys = keys.reshape(N, key_len, <span class="variable language_">self</span>.heads, <span class="variable language_">self</span>.heads_dim)</span><br><span class="line">        query = query.reshape(N, query_len, <span class="variable language_">self</span>.heads, <span class="variable language_">self</span>.heads_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化Q, K, V矩阵</span></span><br><span class="line">        values = <span class="variable language_">self</span>.value(values)</span><br><span class="line">        keys = <span class="variable language_">self</span>.key(keys)</span><br><span class="line">        query = <span class="variable language_">self</span>.query(query)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算注意力分数</span></span><br><span class="line">        <span class="comment"># 这行代码使用爱因斯坦求和约定（Einstein Summation Convention）来计算注意力分数。</span></span><br><span class="line">        <span class="comment"># 具体来说，它通过矩阵乘法计算query和keys之间的点积，然后将其重塑为所需的形状。</span></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        <span class="comment"># 参数解释：</span></span><br><span class="line">        <span class="comment"># - "nqhd,nkhd-&gt;nhqk" 是爱因斯坦求和约定的字符串表示。</span></span><br><span class="line">        <span class="comment">#   - "n" 表示批次大小（batch size）。</span></span><br><span class="line">        <span class="comment">#   - "q" 表示查询序列的长度（query length）。</span></span><br><span class="line">        <span class="comment">#   - "h" 表示注意力头数（number of heads）。</span></span><br><span class="line">        <span class="comment">#   - "d" 表示每个头的维度（dimension per head）。</span></span><br><span class="line">        <span class="comment">#   - "k" 表示键序列的长度（key length）。</span></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        <span class="comment"># 具体操作：</span></span><br><span class="line">        <span class="comment"># - query 的形状为 (N, query_len, heads, heads_dim)。</span></span><br><span class="line">        <span class="comment"># - keys 的形状为 (N, key_len, heads, heads_dim)。</span></span><br><span class="line">        <span class="comment"># - 通过 torch.einsum 计算 query 和 keys 的点积，结果的形状为 (N, heads, query_len, key_len)。</span></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        <span class="comment"># 计算过程：</span></span><br><span class="line">        <span class="comment"># - 对于每个批次（n），每个头（h），计算 query 和 keys 的点积，得到一个形状为 (query_len, key_len) 的矩阵。</span></span><br><span class="line">        <span class="comment"># - 最终结果是一个形状为 (N, heads, query_len, key_len) 的张量，表示每个查询和每个键之间的注意力分数。</span></span><br><span class="line">        energy = torch.einsum(<span class="string">"nqhd,nkhd-&gt;nhqk"</span>, [query, keys])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用注意力机制</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            energy = energy.masked_fill(mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">'-1e20'</span>))</span><br><span class="line"></span><br><span class="line">        attention = torch.softmax(energy / (<span class="variable language_">self</span>.embed_size ** (<span class="number">1</span>/<span class="number">2</span>)), dim=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        out = torch.einsum(<span class="string">"nhql,nlhd-&gt;nqhd"</span>, [attention, values]).reshape(</span><br><span class="line">            N, query_len, <span class="variable language_">self</span>.heads * <span class="variable language_">self</span>.heads_dim</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        out = <span class="variable language_">self</span>.fc_out(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-uncased'</span>, cache_dir=<span class="string">'./cache'</span>)</span><br><span class="line"></span><br><span class="line">    input_text = <span class="string">"Hello world!"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化随机keys,values, query</span></span><br><span class="line"></span><br><span class="line">    tokens = tokenizer.tokenize(input_text)</span><br><span class="line">    token_ids = tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line">    token_ids = torch.tensor([token_ids]).long()</span><br><span class="line"></span><br><span class="line">    batch_size,seq_length = token_ids.shape</span><br><span class="line">    embed_size =<span class="number">512</span></span><br><span class="line">    heads = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">    values = torch.rand(batch_size, seq_length, embed_size)</span><br><span class="line">    keys = torch.rand(batch_size, seq_length, embed_size)</span><br><span class="line">    query = torch.rand(batch_size, seq_length, embed_size)</span><br><span class="line">    mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    attention = MultiHeadAttention(embed_size, heads)</span><br><span class="line">    out = attention(query, keys, values, mask)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Tokens: <span class="subst">{tokens}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Token IDs: <span class="subst">{token_ids}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(out.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Multi-head Attention Output: \n<span class="subst">{out}</span>"</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>运行以上代码后输出 : </p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Tokens: [<span class="string">'hello'</span>, <span class="string">'world'</span>, <span class="string">'!'</span>]</span><br><span class="line">Token IDs: tensor([[<span class="number">7592</span>, <span class="number">2088</span>,  <span class="number">999</span>]])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">3</span>, <span class="number">512</span>])</span><br><span class="line">Multi-head Attention Output:</span><br><span class="line">tensor([[[-<span class="number">0.0855</span>, -<span class="number">0.2076</span>,  <span class="number">0.0354</span>,  ...,  <span class="number">0.1631</span>, -<span class="number">0.0024</span>, -<span class="number">0.1372</span>],</span><br><span class="line">         [-<span class="number">0.0852</span>, -<span class="number">0.2079</span>,  <span class="number">0.0366</span>,  ...,  <span class="number">0.1636</span>, -<span class="number">0.0025</span>, -<span class="number">0.1374</span>],</span><br><span class="line">         [-<span class="number">0.0860</span>, -<span class="number">0.2076</span>,  <span class="number">0.0364</span>,  ...,  <span class="number">0.1644</span>, -<span class="number">0.0018</span>, -<span class="number">0.1361</span>]]],</span><br><span class="line">       grad_fn=&lt;ViewBackward0&gt;)</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="前馈神经网络-1"><a href="#前馈神经网络-1" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h2><p>就是一个简单的全连接层… 没啥好说的，看代码 : </p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- encoding: utf-8 -*-</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">@File    :   FeedForwardNetwork.py</span></span><br><span class="line"><span class="string">@Time    :   2024/09/27 14:46:48</span></span><br><span class="line"><span class="string">@Author  :   pan binghong </span></span><br><span class="line"><span class="string">@Email   :   19909442097@163.com</span></span><br><span class="line"><span class="string">@description   :   </span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"></span><br><span class="line">file_path = os.path.abspath(__file__)</span><br><span class="line"><span class="built_in">dir</span> = os.path.dirname(file_path)</span><br><span class="line">os.chdir(<span class="built_in">dir</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForwardNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, hidden_size, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param d_model:输入的特征维度大小</span></span><br><span class="line"><span class="string">        :param hidden_size:隐藏层大小</span></span><br><span class="line"><span class="string">        :param dropout:dropout概率</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="built_in">super</span>(FeedForwardNetwork, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.liner1 = nn.Linear(d_model, hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.liner2 = nn.Linear(hidden_size, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param x:输入的特征</span></span><br><span class="line"><span class="string">        :return:输出的特征</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        x = <span class="variable language_">self</span>.liner1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.relu(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.liner2(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.dropout(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-uncased'</span>,cache_dir=<span class="string">'./cache'</span>)</span><br><span class="line">    input_text = <span class="string">"Hello world"</span></span><br><span class="line">    tokens = tokenizer.tokenize(input_text)</span><br><span class="line">    token_ids = tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line">    token_ids = torch.tensor([token_ids]).long()</span><br><span class="line"></span><br><span class="line">    d_model = token_ids.shape[<span class="number">1</span>]</span><br><span class="line">    hidden_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line">    ff_netword = FeedForwardNetwork(d_model, hidden_size)</span><br><span class="line"></span><br><span class="line">    output = ff_netword(token_ids.<span class="built_in">float</span>())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'Input tokens: \n<span class="subst">{tokens}</span>'</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'Input token_ids: \n<span class="subst">{token_ids}</span>'</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'Output from FeedForwardNetwork: \n<span class="subst">{output}</span>'</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'Output shape: \n<span class="subst">{output.shape}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>输出结果 : </p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Input tokens:</span><br><span class="line">[<span class="string">'hello'</span>, <span class="string">'world'</span>]</span><br><span class="line">Input token_ids:</span><br><span class="line">tensor([[<span class="number">7592</span>, <span class="number">2088</span>]])</span><br><span class="line">Output <span class="keyword">from</span> FeedForwardNetwork:</span><br><span class="line">tensor([[<span class="number">2332.0767</span>, <span class="number">1194.7576</span>]], grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line">Output shape:</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">2</span>])</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="层归一化-残差连接-1"><a href="#层归一化-残差连接-1" class="headerlink" title="层归一化 &amp; 残差连接"></a>层归一化 &amp; 残差连接</h2><h3 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><p>代码如下 : </p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- encoding: utf-8 -*-</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">@File    :   LayerNormalization.py</span></span><br><span class="line"><span class="string">@Time    :   2024/09/27 15:17:48</span></span><br><span class="line"><span class="string">@Author  :   pan binghong </span></span><br><span class="line"><span class="string">@Email   :   19909442097@163.com</span></span><br><span class="line"><span class="string">@description   :   </span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNormalization</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        初始化层归一化模块</span></span><br><span class="line"><span class="string">        :param features: 特征维度大小</span></span><br><span class="line"><span class="string">        :param eps: 防止除零的小常数</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="built_in">super</span>(LayerNormalization, <span class="variable language_">self</span>).__init__()  <span class="comment"># 调用父类nn.Module的初始化方法</span></span><br><span class="line">        <span class="variable language_">self</span>.eps = eps  <span class="comment"># 设置防止除零的小常数</span></span><br><span class="line">        <span class="variable language_">self</span>.gain = nn.Parameter(torch.ones(features))  <span class="comment"># 初始化增益参数，形状为(features,)，初始值为1</span></span><br><span class="line">        <span class="variable language_">self</span>.bias = nn.Parameter(torch.zeros(features))  <span class="comment"># 初始化偏置参数，形状为(features,)，初始值为0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        前向传播函数</span></span><br><span class="line"><span class="string">        :param x: 输入张量，形状为(batch_size, seq_len, features)</span></span><br><span class="line"><span class="string">        :return: 归一化后的输出张量</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># 计算输入张量在最后一个维度上的均值，保持维度</span></span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># 计算输入张量在最后一个维度上的标准差，保持维度</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.gain * (x - mean) / (std + <span class="variable language_">self</span>.eps) + <span class="variable language_">self</span>.bias  <span class="comment"># 应用层归一化公式并返回结果</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    batch_size = <span class="number">1</span></span><br><span class="line">    seq_len = <span class="number">2048</span></span><br><span class="line">    features = <span class="number">4096</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建一个简单的输入张量</span></span><br><span class="line">    x = torch.randn(batch_size, seq_len, features)  <span class="comment"># 随机初始化输入张量</span></span><br><span class="line">    <span class="comment"># 初始化层归一化层</span></span><br><span class="line">    ln = LayerNormalization(features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 应用层归一化</span></span><br><span class="line">    normalized_x = ln(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印原始和归一化后的张量</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"原始输入张量:"</span>)</span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"\n归一化后的输出张量:"</span>)</span><br><span class="line">    <span class="built_in">print</span>(normalized_x)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"\n归一化后的维度:"</span>)</span><br><span class="line">    <span class="built_in">print</span>(normalized_x.shape)</span><br></pre></td></tr></tbody></table></figure>

<p>输出 : </p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">原始输入张量:</span><br><span class="line">tensor([[[ <span class="number">0.7926</span>,  <span class="number">0.9379</span>,  <span class="number">0.9247</span>,  ..., -<span class="number">0.8849</span>,  <span class="number">0.4262</span>, -<span class="number">0.1126</span>],</span><br><span class="line">         [-<span class="number">1.0093</span>, -<span class="number">0.3612</span>, -<span class="number">0.7135</span>,  ...,  <span class="number">1.7116</span>, -<span class="number">0.1164</span>,  <span class="number">0.8453</span>],</span><br><span class="line">         [ <span class="number">1.1906</span>, -<span class="number">0.3223</span>,  <span class="number">0.3553</span>,  ..., -<span class="number">0.0293</span>,  <span class="number">0.6014</span>,  <span class="number">0.3705</span>],</span><br><span class="line">         ...,</span><br><span class="line">         [-<span class="number">0.3714</span>, -<span class="number">0.1437</span>,  <span class="number">1.2062</span>,  ..., -<span class="number">0.7603</span>, -<span class="number">1.2689</span>, -<span class="number">0.4045</span>],</span><br><span class="line">         [-<span class="number">0.6838</span>, -<span class="number">0.5469</span>,  <span class="number">0.2328</span>,  ..., -<span class="number">0.4500</span>, -<span class="number">1.1035</span>, -<span class="number">0.2005</span>],</span><br><span class="line">         [ <span class="number">0.4267</span>, -<span class="number">0.5163</span>,  <span class="number">1.1316</span>,  ..., -<span class="number">0.1339</span>, -<span class="number">0.4004</span>,  <span class="number">0.2841</span>]]])</span><br><span class="line"></span><br><span class="line">归一化后的输出张量:</span><br><span class="line">tensor([[[ <span class="number">0.7959</span>,  <span class="number">0.9418</span>,  <span class="number">0.9285</span>,  ..., -<span class="number">0.8873</span>,  <span class="number">0.4283</span>, -<span class="number">0.1123</span>],</span><br><span class="line">         [-<span class="number">0.9919</span>, -<span class="number">0.3548</span>, -<span class="number">0.7011</span>,  ...,  <span class="number">1.6830</span>, -<span class="number">0.1141</span>,  <span class="number">0.8314</span>],</span><br><span class="line">         [ <span class="number">1.2045</span>, -<span class="number">0.3012</span>,  <span class="number">0.3732</span>,  ..., -<span class="number">0.0096</span>,  <span class="number">0.6181</span>,  <span class="number">0.3883</span>],</span><br><span class="line">         ...,</span><br><span class="line">         [-<span class="number">0.3827</span>, -<span class="number">0.1589</span>,  <span class="number">1.1681</span>,  ..., -<span class="number">0.7650</span>, -<span class="number">1.2649</span>, -<span class="number">0.4152</span>],</span><br><span class="line">         [-<span class="number">0.7084</span>, -<span class="number">0.5709</span>,  <span class="number">0.2130</span>,  ..., -<span class="number">0.4735</span>, -<span class="number">1.1304</span>, -<span class="number">0.2226</span>],</span><br><span class="line">         [ <span class="number">0.3982</span>, -<span class="number">0.5383</span>,  <span class="number">1.0983</span>,  ..., -<span class="number">0.1585</span>, -<span class="number">0.4232</span>,  <span class="number">0.2567</span>]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line"></span><br><span class="line">归一化后的维度:</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">2048</span>, <span class="number">4096</span>])</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="Residual-Connections"><a href="#Residual-Connections" class="headerlink" title="Residual Connections"></a>Residual Connections</h3><p>残差连接代码实现 : </p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- encoding: utf-8 -*-</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">@File    :   ResidualConnection.py</span></span><br><span class="line"><span class="string">@Time    :   2024/09/27 15:33:49</span></span><br><span class="line"><span class="string">@Author  :   pan binghong </span></span><br><span class="line"><span class="string">@Email   :   19909442097@163.com</span></span><br><span class="line"><span class="string">@description   :   </span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> LayerNormalization <span class="keyword">import</span> LayerNormalization</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualConnection</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(ResidualConnection, <span class="variable language_">self</span>).__init__()  <span class="comment"># 正确调用父类构造函数</span></span><br><span class="line">        <span class="variable language_">self</span>.norm = LayerNormalization(size)  <span class="comment"># 在父类构造函数之后设置属性</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="keyword">return</span> x + <span class="variable language_">self</span>.dropout(sublayer(<span class="variable language_">self</span>.norm(x)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    </span><br><span class="line">    size = <span class="number">512</span></span><br><span class="line">    dropout = <span class="number">0.1</span></span><br><span class="line">    </span><br><span class="line">    residual_module = ResidualConnection(size, dropout)</span><br><span class="line"></span><br><span class="line">    x = torch.rand(<span class="number">32</span>, <span class="number">10</span>, size)</span><br><span class="line">    sublayer = nn.Linear(size, size)</span><br><span class="line"></span><br><span class="line">    output = residual_module(x, sublayer)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'output shape: \n<span class="subst">{output.shape}</span>'</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'out: \n<span class="subst">{output}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>输出内容 : </p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">output shape:</span><br><span class="line">torch.Size([<span class="number">32</span>, <span class="number">10</span>, <span class="number">512</span>])</span><br><span class="line">out:</span><br><span class="line">tensor([[[ <span class="number">0.3520</span>,  <span class="number">0.6030</span>, -<span class="number">0.2354</span>,  ...,  <span class="number">0.8682</span>, -<span class="number">0.3730</span>,  <span class="number">0.1524</span>],</span><br><span class="line">         [ <span class="number">1.4186</span>,  <span class="number">0.5724</span>,  <span class="number">0.2079</span>,  ...,  <span class="number">0.4792</span>,  <span class="number">1.2186</span>,  <span class="number">0.6546</span>],</span><br><span class="line">         [ <span class="number">1.0631</span>,  <span class="number">1.0479</span>,  <span class="number">0.4523</span>,  ...,  <span class="number">1.4115</span>,  <span class="number">1.5870</span>,  <span class="number">0.7165</span>],</span><br><span class="line">         ...,</span><br><span class="line">         [ <span class="number">0.0567</span>,  <span class="number">2.4177</span>,  <span class="number">1.6159</span>,  ..., -<span class="number">0.5791</span>,  <span class="number">0.4186</span>,  <span class="number">0.6306</span>],</span><br><span class="line">         [ <span class="number">1.9228</span>,  <span class="number">0.8467</span>,  <span class="number">0.7399</span>,  ..., -<span class="number">0.5388</span>, -<span class="number">0.1463</span>,  <span class="number">1.1880</span>],</span><br><span class="line">         [ <span class="number">0.1197</span>,  <span class="number">0.0182</span>,  <span class="number">0.2941</span>,  ...,  <span class="number">0.5807</span>,  <span class="number">0.3925</span>, -<span class="number">0.4700</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.3333</span>,  <span class="number">0.8267</span>,  <span class="number">0.3082</span>,  ...,  <span class="number">1.3075</span>,  <span class="number">0.2646</span>,  <span class="number">0.4092</span>],</span><br><span class="line">         [ <span class="number">0.7507</span>,  <span class="number">0.9554</span>,  <span class="number">0.2910</span>,  ...,  <span class="number">1.1357</span>,  <span class="number">0.4684</span>,  <span class="number">0.4244</span>],</span><br><span class="line">         [ <span class="number">0.6514</span>,  <span class="number">0.2003</span>, -<span class="number">0.3597</span>,  ...,  <span class="number">0.3598</span>,  <span class="number">0.4869</span>, -<span class="number">0.1992</span>],</span><br><span class="line">         ...,</span><br><span class="line">         [-<span class="number">0.0030</span>,  <span class="number">1.0668</span>,  <span class="number">0.3075</span>,  ...,  <span class="number">0.5691</span>,  <span class="number">0.2380</span>, -<span class="number">0.1247</span>],</span><br><span class="line">         [ <span class="number">0.9135</span>,  <span class="number">0.4341</span>,  <span class="number">0.0337</span>,  ...,  <span class="number">0.3597</span>, -<span class="number">0.7784</span>,  <span class="number">0.8458</span>],</span><br><span class="line">         [-<span class="number">0.1205</span>,  <span class="number">0.6370</span>,  <span class="number">1.0110</span>,  ...,  <span class="number">0.0136</span>,  <span class="number">0.6965</span>,  <span class="number">0.2374</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.7201</span>,  <span class="number">0.9184</span>,  <span class="number">1.4459</span>,  ...,  <span class="number">0.6506</span>, -<span class="number">0.4328</span>,  <span class="number">0.4222</span>],</span><br><span class="line">         [ <span class="number">0.1091</span>, -<span class="number">0.7816</span>,  <span class="number">0.0389</span>,  ...,  <span class="number">0.4128</span>,  <span class="number">0.5077</span>,  <span class="number">1.2345</span>],</span><br><span class="line">         [ <span class="number">0.3344</span>,  <span class="number">0.5098</span>,  <span class="number">0.1903</span>,  ...,  <span class="number">0.4348</span>,  <span class="number">0.1655</span>,  <span class="number">1.0356</span>],</span><br><span class="line">         ...,</span><br><span class="line">         [ <span class="number">0.6577</span>,  <span class="number">1.0894</span>,  <span class="number">0.3509</span>,  ...,  <span class="number">0.9495</span>, -<span class="number">0.3284</span>,  <span class="number">0.8220</span>],</span><br><span class="line">         [-<span class="number">0.3985</span>,  <span class="number">1.0728</span>,  <span class="number">1.2246</span>,  ...,  <span class="number">0.6893</span>,  <span class="number">0.3443</span>,  <span class="number">0.4279</span>],</span><br><span class="line">         [ <span class="number">0.2423</span>,  <span class="number">0.0113</span>,  <span class="number">1.3485</span>,  ..., -<span class="number">0.2677</span>, -<span class="number">0.0577</span>,  <span class="number">0.4010</span>]],</span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.4007</span>,  <span class="number">1.3420</span>,  <span class="number">0.1977</span>,  ...,  <span class="number">0.8533</span>,  <span class="number">0.1814</span>,  <span class="number">0.2697</span>],</span><br><span class="line">         [ <span class="number">0.6700</span>,  <span class="number">0.2914</span>,  <span class="number">0.7087</span>,  ...,  <span class="number">0.4371</span>, -<span class="number">0.6651</span>, -<span class="number">0.3476</span>],</span><br><span class="line">         [ <span class="number">1.7371</span>, -<span class="number">0.3646</span>, -<span class="number">0.7164</span>,  ..., -<span class="number">0.2941</span>,  <span class="number">1.5312</span>,  <span class="number">0.1496</span>],</span><br><span class="line">         ...,</span><br><span class="line">         [ <span class="number">1.4492</span>,  <span class="number">1.6843</span>,  <span class="number">0.4061</span>,  ...,  <span class="number">0.2602</span>,  <span class="number">0.3412</span>,  <span class="number">0.9145</span>],</span><br><span class="line">         [ <span class="number">0.5208</span>,  <span class="number">0.7262</span>,  <span class="number">1.1507</span>,  ...,  <span class="number">1.6124</span>,  <span class="number">0.1670</span>, -<span class="number">0.7637</span>],</span><br><span class="line">         [ <span class="number">0.6195</span>,  <span class="number">1.8701</span>,  <span class="number">0.6011</span>,  ...,  <span class="number">0.7136</span>,  <span class="number">0.1405</span>,  <span class="number">0.7195</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.2362</span>,  <span class="number">0.3252</span>,  <span class="number">1.5944</span>,  ..., -<span class="number">0.6239</span>, -<span class="number">0.5983</span>,  <span class="number">0.0794</span>],</span><br><span class="line">         [ <span class="number">1.1429</span>, -<span class="number">0.8601</span>,  <span class="number">0.6993</span>,  ...,  <span class="number">0.1767</span>, -<span class="number">0.7440</span>,  <span class="number">0.6210</span>],</span><br><span class="line">         [ <span class="number">0.1337</span>, -<span class="number">0.8456</span>, -<span class="number">0.4567</span>,  ...,  <span class="number">1.0074</span>,  <span class="number">0.6997</span>, -<span class="number">0.7483</span>],</span><br><span class="line">         ...,</span><br><span class="line">         [-<span class="number">0.2421</span>,  <span class="number">0.8441</span>,  <span class="number">1.1355</span>,  ...,  <span class="number">1.2241</span>,  <span class="number">0.0689</span>, -<span class="number">0.8084</span>],</span><br><span class="line">         [ <span class="number">0.1203</span>, -<span class="number">0.1496</span>, -<span class="number">0.3044</span>,  ...,  <span class="number">0.2365</span>,  <span class="number">1.0541</span>,  <span class="number">0.5421</span>],</span><br><span class="line">         [ <span class="number">0.7855</span>,  <span class="number">0.0565</span>,  <span class="number">0.9192</span>,  ...,  <span class="number">0.8071</span>, -<span class="number">0.9707</span>,  <span class="number">1.3335</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.6022</span>,  <span class="number">1.4715</span>,  <span class="number">0.2470</span>,  ..., -<span class="number">0.0782</span>, -<span class="number">0.6734</span>,  <span class="number">0.8383</span>],</span><br><span class="line">         [ <span class="number">0.8088</span>,  <span class="number">0.3382</span>,  <span class="number">0.6812</span>,  ...,  <span class="number">1.1501</span>,  <span class="number">1.0537</span>,  <span class="number">0.5442</span>],</span><br><span class="line">         [-<span class="number">0.0593</span>,  <span class="number">1.7771</span>,  <span class="number">0.0580</span>,  ..., -<span class="number">0.0578</span>,  <span class="number">0.7382</span>,  <span class="number">1.2158</span>],</span><br><span class="line">         ...,</span><br><span class="line">         [ <span class="number">1.1114</span>,  <span class="number">0.3564</span>,  <span class="number">0.8435</span>,  ...,  <span class="number">0.1796</span>,  <span class="number">1.2682</span>,  <span class="number">0.5146</span>],</span><br><span class="line">         [ <span class="number">1.0304</span>,  <span class="number">1.2170</span>,  <span class="number">0.8374</span>,  ...,  <span class="number">2.2357</span>,  <span class="number">0.2286</span>,  <span class="number">0.3899</span>],</span><br><span class="line">         [ <span class="number">0.5022</span>,  <span class="number">0.3711</span>,  <span class="number">0.2397</span>,  ...,  <span class="number">0.9505</span>,  <span class="number">0.3877</span>, -<span class="number">0.2048</span>]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></tbody></table></figure>


<h2 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h2><h3 id="Encoder-Layer"><a href="#Encoder-Layer" class="headerlink" title="Encoder Layer"></a>Encoder Layer</h3><p>编码器层由之前构建的<code>多头注意力机制</code> , <code>前馈神经网络</code> , <code>残差模块</code> , <code>层归一化</code>组合构成</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- encoding: utf-8 -*-</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">@File    :   EncoderLayer.py</span></span><br><span class="line"><span class="string">@Time    :   2024/09/27 16:51:01</span></span><br><span class="line"><span class="string">@Author  :   pan binghong </span></span><br><span class="line"><span class="string">@Email   :   19909442097@163.com</span></span><br><span class="line"><span class="string">@description   :   多头注意力机制、前馈神经网络、位置编码、残差连接和层归一化结合起来，\n</span></span><br><span class="line"><span class="string">                   构建一个 Encoder Layer。Encoder Layer 是 Transformer 编码器的基本组成单位。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> MultiHeadAttention <span class="keyword">import</span> MultiHeadAttention</span><br><span class="line"><span class="keyword">from</span> FeedForwardNetwork <span class="keyword">import</span> FeedForwardNetwork</span><br><span class="line"><span class="keyword">from</span> LayerNormalization <span class="keyword">import</span> LayerNormalization <span class="comment">#在残差连接模块中完成</span></span><br><span class="line"><span class="keyword">from</span> ResidualConnection <span class="keyword">import</span> ResidualConnection</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads, hidden_size, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line">        <span class="variable language_">self</span>.self_attn = MultiHeadAttention(d_model, num_heads)</span><br><span class="line">        <span class="variable language_">self</span>.pos_ffn = FeedForwardNetwork(d_model, hidden_size, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.residual = nn.ModuleList([</span><br><span class="line">            ResidualConnection(d_model, dropout),</span><br><span class="line">            ResidualConnection(d_model, dropout)</span><br><span class="line">        ])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.residual[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: <span class="variable language_">self</span>.self_attn(x, x, x, mask))</span><br><span class="line">        x = <span class="variable language_">self</span>.residual[<span class="number">1</span>](x, <span class="variable language_">self</span>.pos_ffn)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 示例</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 示例输入</span></span><br><span class="line">    d_model = <span class="number">512</span></span><br><span class="line">    num_heads = <span class="number">8</span></span><br><span class="line">    hidden_size = <span class="number">2048</span></span><br><span class="line">    dropout = <span class="number">0.1</span></span><br><span class="line">    batch_size = <span class="number">32</span></span><br><span class="line">    seq_len = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    encoder_layer = EncoderLayer(d_model, num_heads, hidden_size, dropout)</span><br><span class="line">    input_tensor = torch.randn(batch_size, seq_len, d_model)</span><br><span class="line">    mask = torch.ones(batch_size, <span class="number">1</span>,seq_len, seq_len)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    output_tensor = encoder_layer(input_tensor, mask)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出结果</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Input shape:"</span>, input_tensor.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Output shape:"</span>, output_tensor.shape)</span><br></pre></td></tr></tbody></table></figure>

<p>输出结果为 : </p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Input shape: torch.Size([<span class="number">32</span>, <span class="number">10</span>, <span class="number">512</span>])</span><br><span class="line">Output shape: torch.Size([<span class="number">32</span>, <span class="number">10</span>, <span class="number">512</span>])</span><br></pre></td></tr></tbody></table></figure>

<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>Encoder 由 n 个 Encoder_Lyaer 组成，详细代码如下 :</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- encoding: utf-8 -*-</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">@File    :   Encoder.py</span></span><br><span class="line"><span class="string">@Time    :   2024/09/29 08:39:15</span></span><br><span class="line"><span class="string">@Author  :   pan binghong </span></span><br><span class="line"><span class="string">@Email   :   19909442097@163.com</span></span><br><span class="line"><span class="string">@description   :   </span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> EncoderLayer <span class="keyword">import</span> EncoderLayer</span><br><span class="line"><span class="keyword">from</span> LayerNormalization <span class="keyword">import</span> LayerNormalization</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder_layer, num_layers</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder_layer = nn.ModuleList([</span><br><span class="line">            encoder_layer <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">        ])</span><br><span class="line">        <span class="variable language_">self</span>.norm = LayerNormalization(encoder_layer.d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.encoder_layer:</span><br><span class="line">            src = layer(src, mask)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.norm(src)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    d_model = <span class="number">512</span></span><br><span class="line">    num_heades = <span class="number">8</span></span><br><span class="line">    hidden_size = <span class="number">2048</span></span><br><span class="line">    droupout = <span class="number">0.1</span></span><br><span class="line">    num_layers = <span class="number">6</span></span><br><span class="line"></span><br><span class="line">    encoder_layer = EncoderLayer(d_model, num_heades, hidden_size, droupout)</span><br><span class="line">    encoder = Encoder(encoder_layer, num_layers)</span><br><span class="line"></span><br><span class="line">    src = torch.rand(<span class="number">32</span>, <span class="number">10</span> , d_model)</span><br><span class="line"></span><br><span class="line">    output = encoder(src)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></tbody></table></figure>

<p>输出结果为 : </p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Input shape: torch.Size([32, 10, 512])</span><br><span class="line">Output shape: torch.Size([32, 10, 512])</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><p>32 个样本</p>
</li>
<li><p>每个样本有 10 个时间步</p>
</li>
<li><p>每个时间步的特征向量有 512 个维度</p>
</li>
</ul>
<hr>
<h2 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h2><h3 id="Decoder-Layer"><a href="#Decoder-Layer" class="headerlink" title="Decoder_Layer"></a>Decoder_Layer</h3><p>编码器与解码器最大的区别就是使用了 Mask Multi-Head Attention, <strong>用于防止模型训练过程中” 看到” 后续的目标词</strong> , 由多个解码器层构成，代码如下 : </p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- encoding: utf-8 -*-</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">@File    :   DecoderLayer.py</span></span><br><span class="line"><span class="string">@Time    :   2024/09/29 09:34:04</span></span><br><span class="line"><span class="string">@Author  :   pan binghong </span></span><br><span class="line"><span class="string">@Email   :   19909442097@163.com</span></span><br><span class="line"><span class="string">@description   :   Transformer 解码器与编码器类似，\n</span></span><br><span class="line"><span class="string">主要区别在于解码器使用了 Masked Multi-Head Attention，\n</span></span><br><span class="line"><span class="string">用于防止模型在训练过程中“看到”后续的目标词。\n</span></span><br><span class="line"><span class="string">解码器也是由多个 Decoder Layer 堆叠组成。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> MultiHeadAttention <span class="keyword">import</span> MultiHeadAttention</span><br><span class="line"><span class="keyword">from</span> FeedForwardNetwork <span class="keyword">import</span> FeedForwardNetwork</span><br><span class="line"><span class="keyword">from</span> ResidualConnection <span class="keyword">import</span> ResidualConnection</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, heads, hidden_size, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.self_attn = MultiHeadAttention(d_model, heads)</span><br><span class="line">        <span class="variable language_">self</span>.src_attn = MultiHeadAttention(d_model, heads)</span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = FeedForwardNetwork(d_model, hidden_size, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.residuals = nn.ModuleList([</span><br><span class="line">            ResidualConnection(d_model, dropout),</span><br><span class="line">            ResidualConnection(d_model, dropout),            </span><br><span class="line">            ResidualConnection(d_model, dropout)</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask=<span class="literal">None</span>, trg_mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 自注意力机制</span></span><br><span class="line">        x = <span class="variable language_">self</span>.residuals[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: <span class="variable language_">self</span>.self_attn(x, x, x, trg_mask))</span><br><span class="line">        <span class="comment"># 编码器-解码器注意力机制</span></span><br><span class="line">        x = <span class="variable language_">self</span>.residuals[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: <span class="variable language_">self</span>.src_attn(x, memory, memory, src_mask))</span><br><span class="line">        <span class="comment"># 前馈网络</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.residuals[<span class="number">2</span>](x, <span class="variable language_">self</span>.feed_forward)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    embedding_size = <span class="number">512</span></span><br><span class="line">    heads = <span class="number">8</span></span><br><span class="line">    hidden_size = <span class="number">2048</span></span><br><span class="line">    batch_size = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">    decoder_layer = DecoderLayer(embedding_size, heads, hidden_size)</span><br><span class="line">    <span class="built_in">print</span>(decoder_layer)</span><br><span class="line">    <span class="built_in">print</span>(decoder_layer.parameters)</span><br><span class="line"></span><br><span class="line">    x = torch.rand(batch_size, <span class="number">16</span>, embedding_size)</span><br><span class="line">    memory = torch.rand(batch_size, <span class="number">16</span>, embedding_size)</span><br><span class="line">    src_mask = torch.rand(batch_size, <span class="number">1</span>, <span class="number">16</span>)</span><br><span class="line">    trg_mask = torch.rand(batch_size, <span class="number">16</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    output = decoder_layer(x, memory, src_mask, trg_mask)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></tbody></table></figure>

<p>输出结果如下 : </p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">DecoderLayer(</span><br><span class="line">  (self_attn): MultiHeadAttention(</span><br><span class="line">    (value): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    (key): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    (query): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    (fc_out): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (src_attn): MultiHeadAttention(</span><br><span class="line">    (value): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    (key): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    (query): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    (fc_out): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (feed_forward): FeedForwardNetwork(</span><br><span class="line">    (liner1): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">2048</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (relu): ReLU()</span><br><span class="line">    (liner2): Linear(in_features=<span class="number">2048</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (dropout): Dropout(p=<span class="number">0.1</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (residuals): ModuleList(</span><br><span class="line">    (<span class="number">0</span>-<span class="number">2</span>): <span class="number">3</span> x ResidualConnection(</span><br><span class="line">      (norm): LayerNormalization()</span><br><span class="line">      (dropout): Dropout(p=<span class="number">0.1</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">&lt;bound method Module.parameters of DecoderLayer(</span><br><span class="line">  (self_attn): MultiHeadAttention(</span><br><span class="line">    (value): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    (key): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    (query): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    (fc_out): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (src_attn): MultiHeadAttention(</span><br><span class="line">    (value): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    (key): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    (query): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">64</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    (fc_out): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (feed_forward): FeedForwardNetwork(</span><br><span class="line">    (liner1): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">2048</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (relu): ReLU()</span><br><span class="line">    (liner2): Linear(in_features=<span class="number">2048</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (dropout): Dropout(p=<span class="number">0.1</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (residuals): ModuleList(</span><br><span class="line">    (<span class="number">0</span>-<span class="number">2</span>): <span class="number">3</span> x ResidualConnection(</span><br><span class="line">      (norm): LayerNormalization()</span><br><span class="line">      (dropout): Dropout(p=<span class="number">0.1</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)&gt;</span><br><span class="line">torch.Size([<span class="number">8</span>, <span class="number">16</span>, <span class="number">512</span>])</span><br></pre></td></tr></tbody></table></figure>

<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>代码报错解决中…</p>
<hr>
<blockquote>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762">Attention Is All You Need</a></p>
<p><a target="_blank" rel="noopener" href="https://transformers.run/">Transformer</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_45965387/article/details/136862722">从 0 到 1 手撕 Transformer 代码 - 抽丝剥茧 Encoder (Pytorch)</a></p>
</blockquote>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>可怜可怜作者吧！</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.png" alt="潘秉宏 微信">
        <span>微信</span>
      </div>
      <div>
        <img src="/images/alipay.png" alt="潘秉宏 支付宝">
        <span>支付宝</span>
      </div>

  </div>
</div>

          <div class="followme">
  <span>欢迎加我微信~</span>

  <div class="social-list">

      <div class="social-item">
          <span class="social-link">
            <span class="icon">
              <i class="fab fa-weixin"></i>
            </span>

            <span class="label">WeChat</span>
          </span>

          <img class="social-item-img" src="/images/Weixin.jpg">
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/python/" rel="tag"># python</a>
              <a href="/tags/LLMs/" rel="tag"># LLMs</a>
              <a href="/tags/transformer/" rel="tag"># transformer</a>
              <a href="/tags/pytorch/" rel="tag"># pytorch</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/09/24/Python/20240924_%E5%88%9D%E8%AF%86Typing%E5%BA%93/" rel="prev" title="Typing 模块的作用">
                  <i class="fa fa-angle-left"></i> Typing 模块的作用
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2021 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">潘秉宏</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">36k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:12</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div><script type="module">import * as THREE from "/lib/three.js"; window.THREE = THREE;</script><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/panpan02222" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  <script src="/js/third-party/pace.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
